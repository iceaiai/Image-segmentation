{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceaiai/Image-segmentation/blob/main/Image%20segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Setup and Imports"
      ],
      "metadata": {
        "id": "MuuMC2cql2PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary libraries and ensure that the environment is correctly set up"
      ],
      "metadata": {
        "id": "kiSKXAei4u2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "# update package\n",
        "yes | pip uninstall pillow\n",
        "yes | pip install pillow\n",
        "\n",
        "# remove unnecessary\n",
        "cd /content\n",
        "rm -rf *\n",
        "\n",
        "# install package\n",
        "pip install opencv-python pillow\n",
        "pip install segmentation_models_pytorch\n",
        "pip install kaggle\n",
        "pip install dropbox\n",
        "pip install scikit-image"
      ],
      "metadata": {
        "id": "5F5XoJ33dohl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic import\n",
        "import  os, sys, time, math, random, math, psutil\n",
        "from    typing                  import  List, Tuple\n",
        "from    dropbox                 import  Dropbox\n",
        "from    tqdm                    import  tqdm\n",
        "from    matplotlib              import  pyplot      as plt\n",
        "import  numpy                                       as np\n",
        "import  pandas                                      as pd\n",
        "import  zipfile\n",
        "import  warnings\n",
        "import  shutil\n",
        "\n",
        "from    skimage                 import  io\n",
        "from    scipy                   import  interpolate\n",
        "from    scipy.interpolate       import  RegularGridInterpolator\n",
        "from    scipy.ndimage           import  generic_filter\n",
        "from    PIL                     import  Image\n",
        "\n",
        "import  torch\n",
        "from    torch                   import  nn\n",
        "from    torch.nn                import  functional  as F\n",
        "import  torch.optim                                 as optim\n",
        "from    torch.utils.data        import  Dataset, DataLoader, TensorDataset, random_split, Subset\n",
        "from    torchvision             import  transforms, models\n",
        "from    torchvision.transforms  import  *\n",
        "import  torchvision.transforms.functional as TF\n",
        "import  kagglehub\n"
      ],
      "metadata": {
        "id": "U5q46gxNjLFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSP6O0wBDHVY",
        "outputId": "00900b21-5b17-474e-ef9a-06f8e1a4ebe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/batuhanyil/electron-microscopy-particle-segmentation?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 113M/113M [00:01<00:00, 113MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Path to dataset files: /root/.cache/kagglehub/datasets/batuhanyil/electron-microscopy-particle-segmentation/versions/1\n",
            "Dataset moved to: /content/electron-microscopy-particle-segmentation\n",
            "images\tsegmaps\n"
          ]
        }
      ],
      "source": [
        "# download database\n",
        "original_path = kagglehub.dataset_download(\"batuhanyil/electron-microscopy-particle-segmentation\") # EMPS\n",
        "print(\"Original Path to dataset files:\", original_path)\n",
        "\n",
        "path = \"/content/electron-microscopy-particle-segmentation\"\n",
        "shutil.move(original_path, path)\n",
        "print(f\"Dataset moved to: {path}\")\n",
        "\n",
        "!ls {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3L4X0GlDk_P",
        "outputId": "aea85001-3664-4a42-ef46-94fdd75e2a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the dataset folder:\n",
            "['segmaps', 'images']\n"
          ]
        }
      ],
      "source": [
        "print(\"Contents of the dataset folder:\")\n",
        "print(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check hardware\n",
        "print(f\"CPU core #:\\t{os.cpu_count()}\")\n",
        "print(f\"CPU threads #:\\t{psutil.cpu_count(logical=True)}\")\n",
        "print(f\"Total memory:\\t\\t{psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"available GPU #:\\t{gpu_count}\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        print(f\"GPU {i+1}:\\t\\t{gpu_name}\")\n",
        "else:\n",
        "    print(\"No available GPU\")"
      ],
      "metadata": {
        "id": "_WRoOshdD8KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Configuration"
      ],
      "metadata": {
        "id": "Kz-zVYYxlukE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class or dictionary to make hyperparameters easily tunable\n",
        "\n",
        "Note: 'image' folder contains SEM/TEM images. 'segmaps' contained the labelled masks.They are also available in our GT github."
      ],
      "metadata": {
        "id": "mLCc9ZWQ4qgh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qReTBew8WF1"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Data parameters\n",
        "    image_folder        = os.path.join(path, 'images')\n",
        "    mask_folder         = os.path.join(path, 'segmaps')\n",
        "    resize              = (512, 512) # it will be very painful if the image height and width are not the same!!!!\n",
        "\n",
        "    # Determine data cleaning directory\n",
        "    patched_image_folder = os.path.join(path, 'images_with_patch')\n",
        "    patched_mask_folder  = os.path.join(path, 'segmaps_with_patch')\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_epochs      = 20\n",
        "    batch_size      = 16\n",
        "    learning_rate   = 2e-4\n",
        "    momentum        = 0.9\n",
        "    weight_decay    = 1.5e-4\n",
        "\n",
        "    # Model parameters\n",
        "    num_classes = 1  # For binary segmentation\n",
        "    kernel_size = 3\n",
        "    stride      = 1\n",
        "    padding     = 1\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "print(f'current device = {config.device}')"
      ],
      "metadata": {
        "id": "C8nhV2QGouFo",
        "outputId": "bdef19a1-9946-4e18-a332-d76dda6eed6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current device = cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40DBeE9DD3cD"
      },
      "source": [
        "# 3.Dataset preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data cleaning"
      ],
      "metadata": {
        "id": "5YWt0QZdibQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Define Classes"
      ],
      "metadata": {
        "id": "lwcn1mXkWCdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Graph():\n",
        "  '''\n",
        "  A class to represent elemntray Graph\n",
        "\n",
        "  Usage:\n",
        "    plot(Graph): plot image\n",
        "    print(Graph): print image info\n",
        "    data[x,y]: The image pixel RGB value at (x,y) s a NumPy array\n",
        "\n",
        "  Attributes:\n",
        "    data (ndarray): The Graph data loaded as a NumPy array (m x n x 3)\n",
        "    boundaries (dict): The boundary of the Graph\n",
        "    vertices (list): The four vertices of the Graph (xmin, ymin, xmax, ymax)\n",
        "  '''\n",
        "\n",
        "  def __init__(self, data: np.ndarray, xy_limits=None):\n",
        "    '''\n",
        "    input:\n",
        "      xy_limits (list): The x y limits of the Graph (x_min, x_max, y_min, y_max)\n",
        "      data (ndarray): The Graph data loaded as a NumPy array\n",
        "      vertices (list): The four vertices of the Graph (xmin, ymin, xmax, ymax)\n",
        "\n",
        "    return:\n",
        "      instance of a Graph\n",
        "    '''\n",
        "    # data info\n",
        "    self.data           = data\n",
        "    self.dtype          = self.data.dtype\n",
        "    self.shape          = self.data.shape\n",
        "    self.size           = self.data.size\n",
        "    self.xy_limits      = xy_limits\n",
        "    self.vertices       = self.get_vertices()\n",
        "    self.boundaries     = self.get_boundaries()\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    graph_info = f'{type(self).__name__} info:\\n' \\\n",
        "                 f'size           = {self.size}\\n' \\\n",
        "                 f'shape          = {self.data.shape}\\n' \\\n",
        "                 f'data type      = {self.data.dtype}\\n' \\\n",
        "                 f'variable type  = {type(image_0.data)}\\n'\\\n",
        "                 f'\\n'\n",
        "    return graph_info\n",
        "\n",
        "  def get_vertices(self) -> list[tuple]:\n",
        "      \"\"\"\n",
        "      Calculate vertices from xylimits.\n",
        "\n",
        "      Returns:\n",
        "          list: List of vertices as list [(x1, y1), (x2, y2), (x3, y3), (x4, y4)].\n",
        "      \"\"\"\n",
        "      if self.xy_limits is None:\n",
        "          # Default boundary (using image shape)\n",
        "          x_min, y_min  = 0, 0\n",
        "          x_max, y_max  = int(self.shape[0]), int(self.shape[1])\n",
        "      else:\n",
        "          # User-provided boundary\n",
        "          x_min = int(self.xy_limits[0])\n",
        "          x_max = int(self.xy_limits[1])\n",
        "          y_min = int(self.xy_limits[2])\n",
        "          y_max = int(self.xy_limits[3])\n",
        "\n",
        "      # Calculate vertices (Clockwise in matrix, counter-clockwise in Graph)\n",
        "      vertices = [(x_min, y_min),\n",
        "                  (x_max, y_min),\n",
        "                  (x_max, y_max),\n",
        "                  (x_min, y_max)]\n",
        "      # update x y limit\n",
        "      self.xy_limits = (x_min, x_max, y_min, y_max)\n",
        "      return vertices\n",
        "\n",
        "  def get_boundaries(self) -> dict:\n",
        "      \"\"\"\n",
        "      Get the boundary of the image.\n",
        "      \"\"\"\n",
        "      boundaries = {}\n",
        "      boundaries['x_min'] = int(self.vertices[0][0])\n",
        "      boundaries['x_max'] = int(self.vertices[1][0])\n",
        "      boundaries['y_min'] = int(self.vertices[0][1])\n",
        "      boundaries['y_max'] = int(self.vertices[2][1])\n",
        "      boundaries['x_len'] = abs(int(boundaries['x_max'] - boundaries['x_min']))\n",
        "      boundaries['y_len'] = abs(int(boundaries['y_max'] - boundaries['y_min']))\n",
        "      boundaries['diagonal'] = np.sqrt(boundaries['x_len']**2 + boundaries['y_len']**2)\n",
        "      return boundaries\n",
        "\n",
        "  def show(self) -> None:\n",
        "      '''\n",
        "      plot the image\n",
        "      '''\n",
        "      plt.imshow(self.data)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "class Patch(Graph):\n",
        "  '''\n",
        "  A class to represent Patch\n",
        "\n",
        "  Input:\n",
        "    xy_limits ([x_min, x_max, y_min, y_max]): The x y limits of the Patch\n",
        "    x_min/ x_max: row of the Patch pixel matrix\n",
        "    y_min/ y_max: column of the Patch pixel matrix\n",
        "    color_mode (str): the coloring mode of the patch, 'black' or 'BinaryInterpolation'\n",
        "\n",
        "  Attributes:\n",
        "    host_graph (Graph): The Graph instance that patching to\n",
        "\n",
        "  Usage:\n",
        "    plot(Patch): plot image\n",
        "    print(Patch): print image info\n",
        "\n",
        "  Note:\n",
        "    Initial Patch will be black,\n",
        "    Patch will be colored after add_patch(Patch)\n",
        "  '''\n",
        "\n",
        "  def __init__(self, xy_limits: list, color_mode: str):\n",
        "\n",
        "    # Basic info\n",
        "    self.xy_limits  = xy_limits\n",
        "    self.vertices   = self.get_vertices()\n",
        "    self.boundaries = self.get_boundaries()\n",
        "\n",
        "    # Iinitialization （as black patch）\n",
        "    self.color_mode = 'black'\n",
        "    self.data       = self.get_colored()\n",
        "    super().__init__(data=self.data, xy_limits=xy_limits)\n",
        "\n",
        "    # Coloring settings (for add_Patch(Patch))\n",
        "    self.color_mode = color_mode\n",
        "    self.host_graph: Graph\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    graph_info = super().__str__()\n",
        "    patch_info = f'boundary       = {self.boundaries}\\n'\\\n",
        "                 f'vertices       = {self.vertices}\\n'\\\n",
        "                 f'\\n'\n",
        "    return graph_info + patch_info\n",
        "\n",
        "  def get_colored(self) -> np.ndarray:\n",
        "\n",
        "    # set ndarry shape for Graph\n",
        "    x_len, y_len    = self.boundaries['x_len'], self.boundaries['y_len']\n",
        "    RGB_shape       = (x_len, y_len, 3)\n",
        "    GrayScale_shape = (x_len, y_len)\n",
        "\n",
        "\n",
        "    if self.color_mode == 'Transparent':\n",
        "      # colored with original inpyt host image\n",
        "      data = self.host_graph.data.copy()\n",
        "      data = self.get_taylored(data) # tayloring the size\n",
        "\n",
        "    elif self.color_mode == 'black':\n",
        "      # colored with pure plack\n",
        "      data = np.zeros(RGB_shape)\n",
        "\n",
        "\n",
        "    elif self.color_mode == 'BinaryInterpolation_4Points':\n",
        "      # colored with Binary interpolation\n",
        "      data = bilinear_interpolation_4Points(img=self.host_graph.data, xy_limits=self.xy_limits)\n",
        "      data = self.get_taylored(data) # tayloring the size\n",
        "\n",
        "    elif self.color_mode == 'BinaryInterpolation_4Bonds':\n",
        "      # colored with Bilinear interpolation\n",
        "      data = bilinear_interpolation_4Bonds(img=self.host_graph.data, xy_limits=self.xy_limits)\n",
        "      data = self.get_taylored(data) # tayloring the size\n",
        "\n",
        "\n",
        "    elif self.color_mode == 'BinaryInterpolation_Sliding':\n",
        "      # colored with Bilinear interpolation\n",
        "      data = bilinear_interpolation_Sliding(img=self.host_graph.data, xy_limits=self.xy_limits)\n",
        "      data = self.get_taylored(data) # tayloring the size\n",
        "\n",
        "\n",
        "    elif self.color_mode == 'GrayScale_background': # for mask only\n",
        "      # colored for Mask only\n",
        "      marker = 0  # colored as background\n",
        "      data = np.full(GrayScale_shape, marker)\n",
        "\n",
        "\n",
        "    elif self.color_mode == 'GrayScale_out': # for mask only\n",
        "      # colored for Mask only\n",
        "      marker = np.max(self.host_graph.data) + self.host_graph.patch_num + 1\n",
        "      # print(marker)\n",
        "      data = np.full(GrayScale_shape, marker)\n",
        "\n",
        "\n",
        "    else:\n",
        "      error_message = f'Color mode:\\n'\\\n",
        "                      f'black, Transparent\\n'\\\n",
        "                      f'BinaryInterpolation_(4oints,4Bonds,Sliding)\\n'\\\n",
        "                      f'GrayScale_(background,out)'\n",
        "      raise ValueError(error_message)\n",
        "\n",
        "\n",
        "    # update\n",
        "    self.data = data\n",
        "\n",
        "    return data\n",
        "\n",
        "  def get_taylored(self, data):\n",
        "    # tayloring the patch to desired size\n",
        "    data = data[self.boundaries['x_min']:self.boundaries['x_max'],\n",
        "                self.boundaries['y_min']:self.boundaries['y_max']]\n",
        "    return data\n",
        "\n",
        "  def show(self) -> None:\n",
        "      '''\n",
        "      plot the Patch itself\n",
        "      '''\n",
        "      plt.imshow(self.data)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "class Image(Graph):\n",
        "  '''\n",
        "  A class to represent image\n",
        "\n",
        "  Usage:\n",
        "    add_patch(Patch): add single patch to the host Image\n",
        "    get_data_with_patch(): get the data with patches\n",
        "\n",
        "  Attributes:\n",
        "    path (str): path to the image\n",
        "    name (str): name of the image\n",
        "    patch_list (list): list of patches of the host Image\n",
        "    patch_num (int): number of patches of the host Image\n",
        "  '''\n",
        "\n",
        "  def __init__(self, path: str):\n",
        "    '''\n",
        "    Parameters:\n",
        "      path (str): path to the image\n",
        "\n",
        "    return:\n",
        "      instance of Image\n",
        "    '''\n",
        "\n",
        "    # Basic info\n",
        "    self.path       = path\n",
        "    self.name       = path.split(\"/\")[-1]\n",
        "    self.data       = io.imread(self.path)\n",
        "\n",
        "    # Initialization\n",
        "    super().__init__(data=self.data)\n",
        "\n",
        "    # Recording patches\n",
        "    self.patch_list      = []  # list of patches of the host Image\n",
        "    self.patch_num       = 0   # number of patches of the host Image\n",
        "    self.data_with_patch = self.data.copy() # Images with patches\n",
        "\n",
        "  def attach_patch(self, patch: Patch):\n",
        "    '''\n",
        "      Add single patch to the host Image\n",
        "    '''\n",
        "    # Assgin the host image to the patch\n",
        "    patch.host_graph = self\n",
        "\n",
        "    # Color the attached patch\n",
        "    patch.get_colored()\n",
        "\n",
        "    # Attach the patch to the host image\n",
        "    self.patch_list.append(patch)\n",
        "    self.patch_num = len(self.patch_list)\n",
        "\n",
        "    # Update the host image after attachment\n",
        "    self.get_data_with_patch()\n",
        "\n",
        "  def get_data_with_patch(self):\n",
        "    '''\n",
        "      Get the Image data with patches\n",
        "    '''\n",
        "\n",
        "    for patch_i in self.patch_list:\n",
        "        # Get patch boundaries\n",
        "        x_min = patch_i.boundaries['x_min']\n",
        "        x_max = patch_i.boundaries['x_max']\n",
        "        y_min = patch_i.boundaries['y_min']\n",
        "        y_max = patch_i.boundaries['y_max']\n",
        "\n",
        "        # Overlay patch data onto the main image data\n",
        "        self.data_with_patch[x_min:x_max, y_min:y_max] = patch_i.data\n",
        "\n",
        "  def show(self, show_pathces=False) -> None:\n",
        "      '''\n",
        "      plot the image\n",
        "\n",
        "      Parameters:\n",
        "      show_pathces (bool): whether to show patches or not\n",
        "      '''\n",
        "      if show_pathces:\n",
        "        self.get_data_with_patch()\n",
        "        plt.imshow(self.data_with_patch)\n",
        "      else:\n",
        "        plt.imshow(self.data)\n",
        "      plt.show()\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    graph_info = super().__str__()\n",
        "    image_info = f'path           = {self.path}\\n' \\\n",
        "                 f'name           = {self.name}\\n'\\\n",
        "                 f'boundary       = {self.boundaries}\\n'\\\n",
        "                 f'vertices       = {self.vertices}\\n'\\\n",
        "                 f'patch_list     = {self.patch_list}\\n'\\\n",
        "                 f'patch_num      = {self.patch_num}\\n'\\\n",
        "                 f'\\n'\n",
        "    return graph_info + image_info\n",
        "\n",
        "\n",
        "class Mask(Image):\n",
        "  '''\n",
        "  A class to represent mask\n",
        "\n",
        "  Usage:\n",
        "    plot(Mask): plot image\n",
        "    print(Mask): print image info\n",
        "    data[x,y]: The image    value at (x,y) s a NumPy\n",
        "  '''\n",
        "  def __init__(self, path: str):\n",
        "    '''\n",
        "    Parameters:\n",
        "      path (str): path to the mask\n",
        "\n",
        "    return:\n",
        "      instance of Mask\n",
        "    '''\n",
        "    super().__init__(path)\n",
        "    pass\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    return super().__str__()"
      ],
      "metadata": {
        "id": "NgKx7JPRWFMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 Define Functions"
      ],
      "metadata": {
        "id": "0u5X6Ho4Wcd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper functions"
      ],
      "metadata": {
        "id": "2U3lmCinXNIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_average_border(image: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Calculates the average RGB values and applies them to the image border. Doing this is to avoid the turbulence from the boarder color (pure white or pure black)\n",
        "\n",
        "  Args:\n",
        "    image: The input RGB image as an ndarray.\n",
        "\n",
        "  Returns:\n",
        "    The image with the average RGB border.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the average RGB values\n",
        "  average_color = np.mean(image, axis=(0, 1), dtype=int)\n",
        "\n",
        "  # Apply the average color to the image border\n",
        "  image[0,  :, :] = average_color  # Top border\n",
        "  image[-1, :, :] = average_color  # Bottom border\n",
        "  image[:,  0, :] = average_color  # Left border\n",
        "  image[:, -1, :] = average_color  # Right border\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def zip_folder(folder_path, output_path):\n",
        "  \"\"\"\n",
        "  Compress folder to zip file\n",
        "  \"\"\"\n",
        "  with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "      for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        zipf.write(file_path, os.path.relpath(file_path, folder_path))"
      ],
      "metadata": {
        "id": "PB-IBHA9WfaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "algorithm for cleaning the labeling (legend, scale bar, notation, title)"
      ],
      "metadata": {
        "id": "RW3FCHXUYEpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bilinear_interpolation_4Points(img: np.ndarray, xy_limits: list):\n",
        "    \"\"\"\n",
        "    Perform bilinear interpolation on an RGB image.\n",
        "    This function processes the entire input image without changing its dimensions.\n",
        "\n",
        "    Parameters:\n",
        "        img (numpy.ndarray): Input image of shape (height, width, 3).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image after bilinear interpolation with the same shape.\n",
        "    \"\"\"\n",
        "    # Fetch info\n",
        "    x_min, x_max, y_min, y_max = xy_limits\n",
        "    width, height = x_max - x_min + 1, y_max - y_min + 1\n",
        "    _, _, channels = img.shape\n",
        "    interpolated_img = np.zeros_like(img) # host image\n",
        "\n",
        "\n",
        "    # Loop over R, G, B channels\n",
        "    for c in range(channels):\n",
        "\n",
        "        # Fetch pixel values from the 4 fixed points\n",
        "        Q11 = img[x_min, y_min, c]  # Top-left\n",
        "        Q12 = img[x_max, y_min, c]  # Top-right\n",
        "        Q21 = img[x_min, y_max, c]  # Bottom-left\n",
        "        Q22 = img[x_max, y_max, c]  # Bottom-right\n",
        "\n",
        "        for x in range(x_min, x_max + 1):\n",
        "            for y in range(y_min, y_max + 1):\n",
        "\n",
        "                # Compute interpolation weights\n",
        "                weight_x = (x - x_min) / width\n",
        "                weight_y = (y - y_min) / height\n",
        "\n",
        "                # Bilinear interpolation formula\n",
        "                T = Q11 * (1-weight_x) + weight_x * Q12\n",
        "                B = Q21 * (1-weight_x) + weight_x * Q22\n",
        "                P = B * weight_y + T * (1-weight_y)\n",
        "\n",
        "                interpolated_img[x, y, c] = P\n",
        "\n",
        "\n",
        "    return interpolated_img\n",
        "\n",
        "\n",
        "# major method\n",
        "def bilinear_interpolation_4Bonds(img: np.ndarray, xy_limits: list):\n",
        "    \"\"\"\n",
        "    Perform bilinear interpolation on an RGB image.\n",
        "    This function processes the entire input image without changing its dimensions.\n",
        "\n",
        "    Note:\n",
        "        This method uses 4 sliding boundaries rather than 4 fixed corner points.\n",
        "        The 4 points are projections of the current point to the top, bottom, left, and right edges.\n",
        "\n",
        "    Parameters:\n",
        "        img (numpy.ndarray): Input image of shape (height, width, 3).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image after bilinear interpolation with the same shape.\n",
        "    \"\"\"\n",
        "    # Initiation\n",
        "    img = img.copy()\n",
        "    x_min, x_max, y_min, y_max = xy_limits\n",
        "    height, width = x_max - x_min + 1, y_max - y_min + 1\n",
        "    _, _, channels = img.shape\n",
        "    interpolated_img = np.zeros_like(img)\n",
        "\n",
        "    # add border (for dealing with rigid huge label)\n",
        "    img = add_average_border(img)\n",
        "\n",
        "     # Loop over R, G, B channels\n",
        "    for c in range(channels):\n",
        "        for x in range(x_min, x_max+1):\n",
        "            for y in range(y_min, y_max+1):\n",
        "\n",
        "                # Compute interpolation weights\n",
        "                weight_x = (x-x_min) / height\n",
        "                weight_y = (y-y_min) / width\n",
        "\n",
        "                # Fetch pixel values from sliding projection points\n",
        "                Q_Top   = img[x_min,  y,      c]  # Top boundary\n",
        "                Q_Bot   = img[x_max,  y,      c]  # Bottom boundary\n",
        "                Q_Left  = img[x,      y_min,  c]  # Left boundary\n",
        "                Q_Right = img[x,      y_max,  c]  # Right boundary\n",
        "\n",
        "                # Bilinear interpolation formula using boundary projections\n",
        "                H = Q_Top * (1 - weight_x) + Q_Bot  * weight_x\n",
        "                V = Q_Left  * (1 - weight_y) + Q_Right * weight_y\n",
        "\n",
        "                interpolated_img[x, y, c] = (H+V)/2\n",
        "\n",
        "    return interpolated_img\n",
        "\n",
        "\n",
        "def bilinear_interpolation_Sliding(img: np.ndarray, xy_limits: list, WindowScale=0.8) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform bilinear interpolation on an RGB image.\n",
        "    This function processes the entire input image without changing its dimensions.\n",
        "\n",
        "    Parameters:\n",
        "        xy_llimits (list): [x_min, x_max, y_min, y_max]\n",
        "        WindowScale (int): the scale of the sliding window, higher means extract nearer neighboring pixels\n",
        "        img (numpy.ndarray): Input image of shape (height, width, 3).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image after bilinear interpolation with the same shape.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch info\n",
        "    x_min, x_max, y_min, y_max = xy_limits\n",
        "    height, width  = x_max - x_min + 1, y_max - y_min + 1\n",
        "    full_height, full_width, channels = img.shape\n",
        "    interpolated_img = np.zeros_like(img) # host image\n",
        "\n",
        "    # Determing sliding window geometry\n",
        "    window_width  = width  // WindowScale\n",
        "    window_height = height // WindowScale\n",
        "\n",
        "    # Loop over R, G, B channels\n",
        "    for c in range(channels):\n",
        "        for x in range(x_min, x_max):\n",
        "            for y in range(y_min, y_max):\n",
        "\n",
        "                # Make sliding window geometry adaptive\n",
        "                window_x_min = max(int(x - 0.5 * window_height), 0)\n",
        "                window_x_max = min(int(x + 0.5 * window_height), full_height-1)\n",
        "                window_y_min = max(int(y - 0.5 * window_width),  0)\n",
        "                window_y_max = min(int(y + 0.5 * window_width),  full_width-1)\n",
        "\n",
        "                # Fetch pixel values from the 4 fixed points\n",
        "                Q11 = img[window_x_min, window_y_min, c]  # Top-left\n",
        "                Q12 = img[window_x_min, window_y_max, c]  # Top-right\n",
        "                Q21 = img[window_x_max, window_y_min, c]  # Bottom-left\n",
        "                Q22 = img[window_x_max, window_y_max, c]  # Bottom-right\n",
        "\n",
        "\n",
        "                # Compute interpolation weights\n",
        "                weight_x = (x - window_x_min) / (window_x_max - window_x_min + 1e-5)  # Add 1e-5 to denominator\n",
        "                weight_y = (y - window_y_min) / (window_y_max - window_y_min + 1e-5)  # Add 1e-5 to denominator\n",
        "\n",
        "                # Bilinear interpolation formula\n",
        "                T = Q11 * (1-weight_x) + weight_x * Q12\n",
        "                B = Q21 * (1-weight_x) + weight_x * Q22\n",
        "                P = B * weight_y + T * (1-weight_y)\n",
        "\n",
        "                interpolated_img[x, y, c] = P\n",
        "\n",
        "\n",
        "    return interpolated_img\n",
        "\n",
        "\n",
        "def bilinear_interpolation_Sliding_Dynamic(img: np.ndarray, xy_limits: list, min_window=5, max_window=20) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform bilinear interpolation on an RGB image with adaptive window scaling based on local variance.\n",
        "    This function processes the entire input image without changing its dimensions.\n",
        "\n",
        "    Parameters:\n",
        "        xy_limits (list): [x_min, x_max, y_min, y_max]\n",
        "        min_window (int): Minimum sliding window size.\n",
        "        max_window (int): Maximum sliding window size.\n",
        "        img (numpy.ndarray): Input image of shape (height, width, 3).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image after bilinear interpolation with the same shape.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch info\n",
        "    x_min, x_max, y_min, y_max = xy_limits\n",
        "    full_height, full_width, channels = img.shape\n",
        "    interpolated_img = np.zeros_like(img)  # Output image\n",
        "\n",
        "    # Function to calculate local variance\n",
        "    def local_variance(window):\n",
        "        return np.var(window)\n",
        "\n",
        "    # Calculate variance map for grayscale image\n",
        "    grayscale = img.mean(axis=2)  # Convert to grayscale\n",
        "    variance_map = generic_filter(grayscale, local_variance, size=5)\n",
        "\n",
        "    # Normalize variance map to dynamic window scale\n",
        "    WindowScale_map = np.clip((variance_map - variance_map.min()) / (variance_map.max() - variance_map.min()) * max_window,\n",
        "                              min_window, max_window)\n",
        "\n",
        "    # Loop over R, G, B channels\n",
        "    for c in range(channels):\n",
        "        for x in range(x_min, x_max + 1):\n",
        "            for y in range(y_min, y_max + 1):\n",
        "\n",
        "                # Dynamically determine the window scale for each pixel\n",
        "                WindowScale = WindowScale_map[x, y]\n",
        "                window_width  = int(WindowScale)\n",
        "                window_height = int(WindowScale)\n",
        "\n",
        "                # Make sliding window geometry adaptive\n",
        "                window_x_min = max(int(x - 0.5 * window_height), 0)\n",
        "                window_y_min = max(int(y - 0.5 * window_width),  0)\n",
        "                window_x_max = min(int(x + 0.5 * window_height), full_height-1)\n",
        "                window_y_max = min(int(y + 0.5 * window_width),  full_width-1)\n",
        "\n",
        "                # Fetch pixel values from the 4 fixed points\n",
        "                Q11 = img[window_x_min, window_y_min, c]  # Top-left\n",
        "                Q21 = img[window_x_max, window_y_min, c]  # Top-right\n",
        "                Q12 = img[window_x_min, window_y_max, c]  # Bottom-left\n",
        "                Q22 = img[window_x_max, window_y_max, c]  # Bottom-right\n",
        "\n",
        "                # Compute interpolation weights\n",
        "                weight_x = (x - window_x_min) / (window_x_max - window_x_min + 1e-5)\n",
        "                weight_y = (y - window_y_min) / (window_y_max - window_y_min + 1e-5)\n",
        "\n",
        "                # Bilinear interpolation formula\n",
        "                T = Q11 * (1 - weight_y) + Q12 * weight_y\n",
        "                B = Q21 * (1 - weight_y) + Q22 * weight_y\n",
        "                P = T * (1 - weight_x) + B * weight_x\n",
        "\n",
        "                interpolated_img[x, y, c] = P\n",
        "\n",
        "    return interpolated_img"
      ],
      "metadata": {
        "id": "4U3Rv5zZXRpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3 Define Database"
      ],
      "metadata": {
        "id": "RzuWWwtLYqnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch Coordinates Data Base"
      ],
      "metadata": {
        "id": "06WXe_QHY2Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patch_Databas:\n",
        "     \"\"\"Database for recording patches vertices and coloring mode\n",
        "     \"\"\"\n",
        "     def __init__(self):\n",
        "          '''\n",
        "          Parameters:\n",
        "          patches_list (dic): {'name': [points_set_1, points_set_2]}\n",
        "          points_set_1 (list): [point_uper_left, point_botom_right]\n",
        "          point (tuple): (x,y)\n",
        "          x,y (float): x,y coordinate in x,y plane (NOT Matrix row column)\n",
        "          '''\n",
        "\n",
        "          # dic for recording patches vertices\n",
        "          self.patches_dic = {\n",
        "               '7a48f5b4d5':[[(0,0),(70,62)],\n",
        "                              [(0,450),(191,508)]],\n",
        "               'aa968ec1b2':[[(0,0),(78,92)],\n",
        "                              [(522,423),(707,499)]],\n",
        "               '5fc8c5b53c':[],\n",
        "               'ce41e4e56d':[[(0,0),(53,103)],\n",
        "                              [(490,397),(737,511)]],\n",
        "               '9c9d1af243':[[(7,35),(51,86)],\n",
        "                              [(0,400),(160,505)]],\n",
        "               '7c5501d292':[[(10,0),(103,130)]],\n",
        "               '62e729e838':[[(0,0),(69,69)],\n",
        "                              [(420,440),(511,511)]],\n",
        "               'a492171fa8':[[(0,0),(49,51)],\n",
        "                              [(361,461),(511,498)],\n",
        "                             [(0,508),(511,528)]],\n",
        "               '296a52bf09':[],\n",
        "               '1cb46bfdd5':[[(0,0),(145,121)],\n",
        "                              [(510,353),(749,511)]],\n",
        "               '99eb4bad25':[[(21,27),(98,89)],\n",
        "                              [(27,420),(167,490)]],\n",
        "\n",
        "               '671b7c8831':[[(568,13),(684,52)],\n",
        "                             [(518,497),(684,511)]],\n",
        "               'f7f7093f0f':[[(606,28),(670,78)],\n",
        "                         [(537,432),(659,488)]],\n",
        "               'a68915823c':[[(0,0),(60,67)],\n",
        "                         [(420,446),(726,508)]],\n",
        "               '555eb05b4c':[[(19,47),(58,89)],\n",
        "                         [(0,476),(682,511)]],\n",
        "               '923cab32ee':[],\n",
        "               '6360a823da':[[(18,24),(82,94)],\n",
        "                         [(18,428),(177,508)]],\n",
        "               'e853d120fa':[[(0,0),(46,50)],\n",
        "                         [(385,456),(510,494)],\n",
        "                         [(0,506),(511,527)]],\n",
        "               '434e287439':[[(16,443),(170,500)]],\n",
        "               '3430b20071':[[(0,0),(73,77)],\n",
        "                         [(12,466),(99,500)]],\n",
        "               '7b76562318':[[(0,12),(48,70)],\n",
        "                         [(0,465),(722,511)]],\n",
        "               'c0ba8a65a5':[[(0,0),(102,101)],\n",
        "                         [(0,425),(186,511)]],\n",
        "               '48f3a148f6':[[(0,0),(61,48)],\n",
        "                         [(359,489),(778,511)]],\n",
        "               'b85f5e414c':[[(15,22),(42,54)],\n",
        "                         [(404,441),(551,511)]],\n",
        "               '391ef00939':[[(16,9),(67,51)],\n",
        "                         [(418,461),(511,511)]],\n",
        "               'a8851978de':[[(413,480),(560,511)]],\n",
        "               'c7e9ec8d48':[[(0,0),(44,58)],\n",
        "                         [(0,419),(150,511)]],\n",
        "               '2897b777fe':[[(0,0),(84,84)],\n",
        "                              [(0,446),(124,511)]],\n",
        "               '3f97a9e821':[[(402,465),(516,506)]],\n",
        "               'dfeaa0d54b':[[(0,0),(72,67)],\n",
        "                         [(23,427),(238,495)]],\n",
        "               'fc2851e6d3':[[(353,449),(477,502)]],\n",
        "               '63df66fb4d':[[(0,473),(84,511)],\n",
        "                         [(547,430),(669,506)]],\n",
        "               '8136144eaf':[[(460,13),(517,63)],\n",
        "                         [(332,475),(554,511)]],\n",
        "               'c294fece54':[[(481,0),(537,58)],\n",
        "                         [(307,473),(558,508)]],\n",
        "               '427ebc9e15':[[(57,452),(675,500)]],\n",
        "               '85f5efb77d':[],\n",
        "               'f7fa7d1729':[[(0,0),(83,78)],\n",
        "                         [(3,438),(143,502)]],\n",
        "               '4628753cd8':[[(0,461),(670,511)]],\n",
        "               '2556313831':[[(0,0),(109,53)],\n",
        "                         [(4,451),(120,511)]],\n",
        "               'c812c93fbd':[[(5,13),(39,62)],\n",
        "                          [(252,448),(546,511)]],\n",
        "               'e3a800b533':[[(28,31),(72,79)],\n",
        "                         [(9,470),(80,507)]],\n",
        "               'a1315df346':[[(28,444),(245,489)]],\n",
        "               'bf901d32a3':[[(0,0),(110,96)],\n",
        "                         [(567,436),(634,493)]],\n",
        "               '80d6a1f25a':[[(0,0),(67,70)],\n",
        "                         [(391,445),(504,494)]],\n",
        "               '707120d0f5':[[(21,435),(331,507)]],\n",
        "               'c1d32ab54d':[],\n",
        "               '398ae34c19':[[(0,0),(63,79)],\n",
        "                         [(536,414),(745,511)]],\n",
        "               'fcc97c015c':[[(0,0),(62,49)],\n",
        "                         [(313,431),(434,468)],\n",
        "                         [(0,476),(634,511)]],\n",
        "               '2daa27a6df':[],\n",
        "               'b452204939':[[(0,0),(41,45)],\n",
        "                         [(0,472),(557,511)]],\n",
        "               '6169a602a0':[],\n",
        "               '5898a2ac02':[[(0,482),(62,511)]],\n",
        "               'aa309f6cec':[[(0,0),(54,48)],\n",
        "                         [(424,452),(511,511)]],\n",
        "               'a4b639ab1a':[[(0,0),(64,91)],\n",
        "                         [(0,451),(128,505)]],\n",
        "               'a3eb38c236':[[(585,25),(650,75)],\n",
        "                         [(0,462),(140,508)]],\n",
        "               'f3b36fb2ab':[],\n",
        "               '1e011bd992':[],\n",
        "               '62594342ac':[[(355,446),(893,509)]],\n",
        "               'a59bcf3f13':[],\n",
        "               '5eb66b74ea':[[(339,429),(511,511)]],\n",
        "               'd933f429cc':[[(6,12),(33,63)],\n",
        "                        [(308,444),(546,511)]],\n",
        "               '22c776b059':[[(0,0),(91,93)],\n",
        "                         [(549,422),(717,491)]],\n",
        "               'ddffc403e3':[[(0,0),(68,68)],\n",
        "                         [(505,449),(647,511)]],\n",
        "               '9fac661e24':[],\n",
        "               '627acee9c4':[[(22,30),(57,66)],\n",
        "                         [(6,463),(88,504)]],\n",
        "               '9246d32fe0':[[(683,23),(738,69)],\n",
        "                         [(8,452),(140,505)]],\n",
        "               'cf6823322f':[[(0,0),(66,73)],\n",
        "                         [(476,427),(632,511)]],\n",
        "               '065d8e352c':[[(363,434),(524,511)]],\n",
        "               '53025d5189':[[(21,13),(55,48)],\n",
        "                         [(6,466),(101,510)]],\n",
        "               'ae843e9fc0':[[(19,67),(73,121)],\n",
        "                         [(317,413),(617,511)]],\n",
        "               '8b40ed2a82':[[(558,14),(624,73)],\n",
        "                         [(4,468),(116,507)]],\n",
        "               '3a8af6d3f4':[[(468,8),(497,41)],\n",
        "                         [(12,479),(104,507)]],\n",
        "               '0e41d62d5d':[[(7,15),(47,47)],\n",
        "                         [(11,470),(90,508)]],\n",
        "               '22d5aab090':[[(684,26),(743,71)],\n",
        "                         [(12,465),(124,506)]],\n",
        "               'd90c9c5b72':[[(0,0),(100,100)],\n",
        "                         [(600,417),(721,471)]],\n",
        "               'e211d9d7ac':[[(0,0),(65,65)],\n",
        "                         [(346,491),(372,511)],\n",
        "                         [(463,493),(660,511)]],\n",
        "               '291898b7c6':[[(0,0),(81,86)],\n",
        "                         [(366,448),(482,502)]],\n",
        "               '41ac55ed12':[[(72,25),(153,97)],\n",
        "                         [(530,415),(661,494)]],\n",
        "               '6e16b7dcb7':[[(0,0),(65,87)],\n",
        "                         [(14,427),(170,505)]],\n",
        "               'ee8a331d06':[[(0,0),(48,35)],\n",
        "                         [(10,473),(73,503)]],\n",
        "               '43875c5b20':[[(0,0),(85,77)],\n",
        "                         [(419,443),(572,504)]],\n",
        "               '441414704c':[[(15,17),(60,64)],\n",
        "                         [(370,451),(511,511)]],\n",
        "               '7701ac03c1':[[(13,46),(66,104)],\n",
        "                         [(0,392),(174,485)]],\n",
        "               '00655d9628':[[(0,40),(60,88)],\n",
        "                         [(419,477),(511,514)]],\n",
        "               '78834ed976':[[(10,26),(45,67)]],\n",
        "               '87429394b0':[[(0,0),(74,73)],\n",
        "                         [(602,426),(747,490)]],\n",
        "               '173c98ef82':[[(5,466),(99,509)]],\n",
        "               'af2a275342':[[(0,0),(86,97)],\n",
        "                         [(6,438),(101,504)]],\n",
        "               'c355821ea6':[[(25,24),(74,56)]],\n",
        "               'f664201c62':[[(411,477),(550,511)]],\n",
        "               'e49e69fd61':[[(15,10),(186,48)],\n",
        "                         [(426,432),(532,500)]],\n",
        "               '82b9b9c2af':[[(15,11),(76,68)],\n",
        "                         [(18,443),(186,503)]],\n",
        "               '4ec7671d11':[[(0,0),(147,104)],\n",
        "                             [(0,473),(629,511)]],\n",
        "               '027f25010a':[[(24,553),(129,600)],\n",
        "                             [(0,0),(12,112)]],\n",
        "               '864a2a6ccf':[[(5,14),(48,60)],\n",
        "                              [(20,400),(200,488)]],\n",
        "               '2ee0518d7a':[],\n",
        "               '2181ebb330':[[(0,0),(83,65)],\n",
        "                              [(0,468),(168,511)]],\n",
        "               '9797ed1000':[[(0,0),(60,83)],\n",
        "                              [(482,447),(665,511)]],\n",
        "               '8b7ce99d43':[[(8,26),(100,100)],\n",
        "                              [(532,423),(648,471)]],\n",
        "               '98a18b7db9':[[(15,21),(104,96)],\n",
        "                             [(583,421),(744,510)]],\n",
        "               'cecb19b837':[[(0,0),(64,73)],\n",
        "                              [(251,0),(511,68)],\n",
        "                              [(341,427),(511,511)]],\n",
        "               '21308b3614':[[(12,16),(86,82)],\n",
        "                              [(0,475),(637,511)]],\n",
        "               '6643f01cf4':[[(25,441),(181,490)]],\n",
        "               'ec2cc16d2a':[[(0,0),(84,74)],\n",
        "                         [(420,404),(620,470)]],\n",
        "               '986f86c3b5':[[(0,0),(61,76)],\n",
        "                         [(0,420),(174,511)]],\n",
        "               '3bbd05202e':[[(13,12),(152,48)],\n",
        "                         [(490,444),(573,489)]],\n",
        "               '84a6330bdb':[[(42,437),(146,503)]],\n",
        "               '77f09d7ca4':[[(0,0),(53,63)]],\n",
        "               '74ef917e76':[[(17,7),(76,60)],\n",
        "                         [(467,451),(723,504)]],\n",
        "               '42750053e0':[[(4,14),(50,54)],\n",
        "                         [(12,428),(130,480)]],\n",
        "               '9d358e8c8a':[],\n",
        "               '78847f96ee':[[(7,17),(77,73)],\n",
        "                         [(327,0),(689,45)],\n",
        "                         [(11,424),(174,491)]],\n",
        "               '39dda96b25':[[(0,0),(50,50)],\n",
        "                         [(366,470),(511,501)],\n",
        "                         [(0,510),(511,527)]],\n",
        "               '544b32e4cd':[[(0,0),(37,37)],\n",
        "                         [(5,483),(72,504)]],\n",
        "               '47286679c2':[[(0,447),(685,511)]],\n",
        "               '5e22377208':[],\n",
        "               '83afcabd95':[[(592,0),(728,153)],\n",
        "                         [(11,454),(119,506)]],\n",
        "               'f6c0d8967e':[],\n",
        "               '39b3a6db5e':[[(0,480),(517,511)]],\n",
        "               '513158cf83':[[(0,0),(85,67)],\n",
        "                         [(5,429),(329,487)]],\n",
        "               '89e4044418':[[(14,437),(148,506)]],\n",
        "               'e1adc2ba25':[[(11,17),(81,93)],\n",
        "                         [(535,430),(688,504)]],\n",
        "               '39b0dc1f52':[[(0,0),(60,63)],\n",
        "                         [(0,460),(680,511)]],\n",
        "               '207bb15702':[[(0,0),(64,97)],\n",
        "                         [(532,427),(742,511)]],\n",
        "               '34864e691e':[[(437,26),(511,93)],\n",
        "                         [(350,435),(477,500)]],\n",
        "               '25af264996':[[(28,424),(202,487)]],\n",
        "               '7e85b64066':[[(11,13),(87,104)],\n",
        "                         [(0,457),(631,511)]],\n",
        "               '80719af02f':[],\n",
        "               '535134e785':[[(11,12),(81,72)],\n",
        "                             [(4,445),(115,510)]],\n",
        "               '0289bd97e6':[[(0,0),(58,64)],\n",
        "                         [(14,435),(144,485)]],\n",
        "               '0144266d21':[[(340,468),(500,500)]],\n",
        "               '4822787326':[],\n",
        "               'd6613c4ec5':[[(0,0),(98,89)],\n",
        "                         [(456,414),(615,511)]],\n",
        "               'b2f6f1bbf9':[[(0,0),(128,107)],\n",
        "                         [(455,444),(560,511)]],\n",
        "               '5a25d179cc':[[(0,478),(732,511)]],\n",
        "               '93ed483d79':[[(26,503),(188,569)]],\n",
        "               'a880175367':[[(357,436),(487,503)]],\n",
        "               '5d72761807':[[(0,0),(221,46)]],\n",
        "               'f7f9125fb0':[[(439,24),(526,90)],\n",
        "                         [(30,417),(152,487)]],\n",
        "               '2dadb1ced1':[[(39,70),(93,135)],\n",
        "                         [(23,448),(165,492)]],\n",
        "               '76dde347fc':[[(465,9),(495,41)],\n",
        "                         [(9,474),(105,506)]],\n",
        "               '5addfe7111':[[(0,0),(64,78)],\n",
        "                         [(3,464),(81,508)]],\n",
        "               '06f605c0b1':[[(0,0),(41,40)],\n",
        "                         [(5,486),(73,509)]],\n",
        "               '0ca41ed5b8':[[(15,47),(114,110)],\n",
        "                         [(8,461),(124,493)]],\n",
        "               '23d581d8a9':[[(0,0),(71,75)],\n",
        "                         [(411,475),(503,496)]],\n",
        "               'c1cccb3332':[[(4,19),(46,64)],\n",
        "                         [(3,463),(101,510)]],\n",
        "               'bfb2a25730':[[(13,22),(84,83)],\n",
        "                         [(8,428),(223,499)]],\n",
        "               'fc9e19325c':[[(6,11),(41,47)],\n",
        "                         [(11,490),(90,529)]],\n",
        "               'b2a7683174':[[(489,11),(516,41)],\n",
        "                         [(211,457),(292,498)]],\n",
        "               '01ac659240':[[(470,451),(551,493)]],\n",
        "               'e4065c11e4':[[(9,8),(59,52)],\n",
        "                         [(428,444),(554,503)]],\n",
        "               '5c244a9ea8':[[(0,466),(668,511)]],\n",
        "               'ce6d981687':[[(12,20),(112,105)],\n",
        "                         [(585,430),(765,511)]],\n",
        "               'a630b192d4':[[(0,0),(63,69)]],\n",
        "               '7fabcfe23f':[[(8,14),(51,55)],\n",
        "                         [(0,477),(633,511)]],\n",
        "               'd28fb3e471':[[(693,0),(739,44)],\n",
        "                         [(0,487),(100,511)]],\n",
        "               'c363833ad2':[[(0,0),(123,103)],\n",
        "                         [(435,444),(527,489)]],\n",
        "               '73e0a09cd2':[[(9,14),(39,51)],\n",
        "                         [(9,465),(90,508)]],\n",
        "               '245e1a4826':[[(0,472),(519,511)]],\n",
        "               'fff35f80e1':[[(6,463),(113,507)]],\n",
        "               '8fde53b05f':[],\n",
        "               'eb67400438':[[(0,0),(31,34)],\n",
        "                         [(6,472),(169,511)]],\n",
        "               '1e0f5d817e':[[(16,15),(101,89)],\n",
        "                         [(33,425),(183,492)]],\n",
        "               '78b42bc5e8':[[(16,28),(65,77)],\n",
        "                         [(515,436),(595,495)]],\n",
        "               'f9d59dcb02':[],\n",
        "               '39e18a8f4c':[[(5,25),(82,94)],\n",
        "                         [(519,438),(617,511)]],\n",
        "               '2e5dab1969':[[(7,21),(52,73)],\n",
        "                         [(13,456),(130,505)]],\n",
        "               '6fe6ed78ab':[[(432,105),(559,218)]],\n",
        "               '7ea28e7e54':[[(11,16),(108,80)],\n",
        "                             [(470,488),(764,510)]],\n",
        "               'd29d644539':[[(0,0),(104,88)],\n",
        "                         [(0,483),(588,511)]],\n",
        "               '3537f150cd':[[(8,468),(212,510)]],\n",
        "               'a8e3f32537':[[(0,0),(117,68)],\n",
        "                         [(580,435),(676,493)]],\n",
        "               'f58490bdaf':[],\n",
        "               'bc5529f7ca':[[(616,0),(680,69)],\n",
        "                         [(0,477),(681,511)]],\n",
        "               '7f50b86c4b':[[(23,12),(81,69)],\n",
        "                         [(343,435),(496,499)]],\n",
        "               'cd22803960':[],\n",
        "               'b9deddff33':[[(381,459),(500,510)]],\n",
        "               'ce2f96aae1':[[(0,0),(52,67)],\n",
        "                         [(445,390),(730,497)]],\n",
        "               '208b16bbb7':[],\n",
        "               '10d85f70bd':[[(0,0),(55,46)],\n",
        "                         [(388,464),(495,510)]],\n",
        "               '4e13b5c5f8':[[(0,0),(74,107)],\n",
        "                         [(385,462),(535,487)]],\n",
        "               '2e4a3ab627':[[(11,32),(72,82)],\n",
        "                         [(0,481),(773,511)]],\n",
        "               '9dc60257e5':[[(6,18),(54,66)],\n",
        "                         [(388,470),(682,511)],\n",
        "                         [(230,396),(454,456)],\n",
        "                         [(0,488),(682,511)]],\n",
        "               '78bda6c73b':[[(0,0),(109,94)],\n",
        "                         [(15,419),(117,504)]],\n",
        "               '2242820128':[[(0,0),(112,119)],\n",
        "                         [(482,470),(681,511)]],\n",
        "               'ed3a3cb76b':[],\n",
        "               '9c68e03e41':[[(6,454),(142,511)]],\n",
        "               '37ce835fb2':[[(0,0),(73,77)],\n",
        "                         [(0,464),(915,511)]],\n",
        "               '6db657d4f7':[[(12,21),(85,85)],\n",
        "                         [(405,454),(491,507)]],\n",
        "               '0a30487a4e':[[(8,12),(66,72)],\n",
        "                         [(419,484),(500,507)]],\n",
        "               '9ca16b98df':[[(449,422),(593,477)]],\n",
        "               'd3ed612a2d':[[(22,34),(66,94)],\n",
        "                         [(8,453),(151,511)]],\n",
        "               '36de69b2c0':[[(9,446),(140,499)]],\n",
        "               'c36b04376a':[[(0,478),(554,511)]],\n",
        "               '2d6f268052':[[(13,12),(65,62)],\n",
        "                         [(16,464),(130,508)]],\n",
        "               '478a6c42ec':[[(2,14),(58,72)],\n",
        "                         [(4,438),(270,511)]],\n",
        "               '90615b870a':[[(0,0),(67,68)],\n",
        "                         [(381,0),(511,68)],\n",
        "                         [(345,429),(511,512)]],\n",
        "               'cf703078f2':[[(638,369),(941,511)]],\n",
        "               '2d1a6c5ce4':[[(4,3),(87,75)],\n",
        "                         [(0,430),(203,511)]],\n",
        "               '782d7f752d':[[(19,31),(47,57)],\n",
        "                         [(572,447),(682,503)]],\n",
        "               '1bf56ce626':[[(0,14),(29,82)],\n",
        "                         [(531,465),(680,511)]],\n",
        "               '611b9564a4':[[(15,15),(86,83)],\n",
        "                         [(391,441),(485,488)]],\n",
        "               'f28b702df5':[[(6,16),(42,55)],\n",
        "                         [(552,452),(663,500)]],\n",
        "               'b16f4f83a4':[[(409,454),(545,511)]],\n",
        "               '58e8237dab':[[(0,0),(87,91)],\n",
        "                         [(599,418),(724,471)]],\n",
        "               '19b0afdf81':[[(15,12),(70,68)],\n",
        "                         [(362,468),(513,502)],\n",
        "                         [(0,452),(237,509)]],\n",
        "               '5737e0c2c7':[[(0,0),(76,65)],\n",
        "                         [(18,458),(156,507)]],\n",
        "               'c3d3764a5a':[],\n",
        "               'e4268d7502':[[(0,0),(76,87)],\n",
        "                         [(384,486),(505,537)]],\n",
        "               'f6f3758bf4':[[(11,29),(50,67)],\n",
        "                         [(6,456),(281,510)]],\n",
        "               '34f4fb273d':[[(347,416),(482,505)]],\n",
        "               'a66c75354a':[],\n",
        "               '913312cfdf':[],\n",
        "               '9fa90de188':[[(0,0),(84,78)],\n",
        "                         [(465,480),(765,509)]],\n",
        "               '0a55e7c93f':[[(48,472),(398,507)]],\n",
        "               '28d7e31165':[[(19,484),(111,505)]],\n",
        "               'fcebbb1fd9':[[(0,16),(45,77)],\n",
        "                         [(0,489),(64,562)]],\n",
        "               '9d2e813224':[[(342,444),(483,483)]],\n",
        "               'f98c5fb82a':[[(18,475),(125,522)]],\n",
        "               'a1dada3898':[[(0,0),(72,69)],\n",
        "                         [(0,477),(654,511)]],\n",
        "               '97e645b56e':[[(0,0),(40,40)],\n",
        "                         [(347,442),(504,505)]],\n",
        "               'cef2196885':[],\n",
        "               'ae690251d5':[],\n",
        "               '75e968af41':[[(0,0),(76,67)],\n",
        "                         [(0,436),(105,509)]],\n",
        "               'bc1fe886af':[[(905,38),(1063,177)],\n",
        "                         [(57,347),(359,480)]],\n",
        "               '43be4d0a2c':[[(588,458),(664,505)]],\n",
        "               'e8e39c913a':[[(0,486),(51,519)]],\n",
        "               'b392e4784f':[[(379,455),(478,498)]],\n",
        "               '60c63e2383':[[(0,0),(89,60)],\n",
        "                         [(14,463),(191,511)]],\n",
        "               'fe76e94abf':[[(0,0),(58,67)],\n",
        "                         [(400,448),(578,511)]],\n",
        "               'f068954421':[[(0,0),(45,47)],\n",
        "                         [(6,407),(135,487)]],\n",
        "               'd4b73860c3':[[(579,12),(686,48)],\n",
        "                             [(523,504),(689,511)]],\n",
        "               '586dc6cc59':[[(18,424),(113,489)]],\n",
        "               '3a9246e0ee':[[(0,0),(78,69)],\n",
        "                         [(470,485),(762,506)]],\n",
        "               '738c326d73':[[(0,0),(91,65)],\n",
        "                         [(25,439),(148,494)]],\n",
        "               'c6ea896731':[[(33,0),(106,93)],\n",
        "                         [(543,408),(739,504)]],\n",
        "               '88808139d3':[[(0,0),(80,93)],\n",
        "                         [(445,474),(681,511)],\n",
        "                         [(2,487),(212,511)]],\n",
        "               'f6e0ae6378':[[(0,0),(60,65)],\n",
        "                         [(530,415),(738,511)]],\n",
        "               '636334b3f1':[],\n",
        "               '8e50748a7b':[[(0,0),(79,59)]],\n",
        "               'fe9791f122':[[(1,14),(28,42)],\n",
        "                         [(570,472),(650,511)]],\n",
        "               '5c418c2eb0':[[(0,0),(112,99)],\n",
        "                         [(517,437),(619,496)]],\n",
        "               '4bafa92eec':[[(406,429),(551,511)]],\n",
        "               '70f9f13af4':[[(2,22),(41,72)],\n",
        "                         [(488,422),(630,478)]],\n",
        "               'a562af31ea':[[(5,17),(32,65)],\n",
        "                         [(428,433),(578,500)]],\n",
        "               '31f17306c8':[[(0,469),(726,511)]],\n",
        "               '1d67fb3321':[[(5,37),(72,86)],\n",
        "                         [(412,467),(511,511)]],\n",
        "               '763c317d5d':[[(0,0),(83,65)],\n",
        "                         [(0,432),(109,508)]],\n",
        "               '9318f3ce70':[[(19,15),(67,73)]],\n",
        "               '5a65f9eba2':[[(9,13),(81,110)],\n",
        "                         [(14,422),(112,507)]],\n",
        "               '1b935635dd':[[(12,24),(103,103)],\n",
        "                         [(437,432),(696,511)]],\n",
        "               'dc25ff5266':[[(12,12),(67,63)],\n",
        "                         [(573,454),(630,490)]],\n",
        "               'f78844f8ce':[[(6,18),(44,65)],\n",
        "                         [(9,447),(86,497)]],\n",
        "               'ca780314cd':[[(14,29),(61,76)],\n",
        "                         [(543,426),(731,511)]],\n",
        "               '74391d74bd':[[(47,36),(130,103)],\n",
        "                         [(22,433),(232,498)]],\n",
        "               '2b9ccd5679':[[(16,8),(46,42)],\n",
        "                         [(524,418),(656,492)]],\n",
        "               '1736c60650':[[(0,0),(119,46)],\n",
        "                         [(328,427),(614,511)]],\n",
        "               'ac49e14130':[[(8,23),(69,76)],\n",
        "                         [(24,451),(131,503)]],\n",
        "               '59424b060a':[[(0,0),(37,47)],\n",
        "                         [(467,492),(511,511)]],\n",
        "               '96c772f28b':[[(13,16),(85,86)],\n",
        "                         [(0,476),(682,511)]],\n",
        "               '406d70e411':[[(0,0),(58,44)],\n",
        "                         [(0,472),(670,511)]],\n",
        "               'bbb27830b9':[[(63,66),(95,114)]],\n",
        "               'e83de4878a':[[(0,0),(64,68)],\n",
        "                         [(463,0),(740,151)],\n",
        "                         [(500,412),(725,508)]],\n",
        "               'd7da1a24ad':[[(0,0),(63,64)],\n",
        "                         [(26,442),(261,492)]],\n",
        "               '31a727b95d':[[(14,22),(77,66)]],\n",
        "               '5568be0f4c':[[(0,0),(76,65)],\n",
        "                         [(7,432),(160,503)]],\n",
        "               'b996f341bd':[[(0,0),(88,88)],\n",
        "                         [(547,422),(729,511)]],\n",
        "               'af124a45d9':[[(0,482),(723,511)]],\n",
        "               '26e5f24689':[[(3,465),(134,510)]],\n",
        "               '6de290c824':[[(11,11),(100,41)],\n",
        "                         [(15,434),(119,497)]],\n",
        "               '3027f9ffe7':[[(0,472),(686,511)]],\n",
        "               '31121ca5cf':[[(15,13),(69,53)],\n",
        "                         [(417,459),(507,505)]],\n",
        "               'f8a1b2bf39':[[(0,0),(76,93)],\n",
        "                         [(0,474),(683,511)]],\n",
        "               '60feb7de8f':[[(0,451),(690,511)]],\n",
        "               'af674cfe7e':[],\n",
        "               '7f94f4bdab':[[(0,0),(87,71)],\n",
        "                         [(593,429),(738,495)]],\n",
        "               '04dbda93f9':[[(402,447),(505,499)]],\n",
        "               '3d1937b05c':[[(7,21),(57,66)],\n",
        "                         [(618,413),(776,511)]],\n",
        "               '8d7fc4c578':[[(14,18),(65,58)],\n",
        "                         [(0,478),(661,511)]],\n",
        "               '802e607c7c':[[(17,26),(87,80)],\n",
        "                         [(2,448),(157,511)]],\n",
        "               'c83a6e91b0':[[(0,0),(94,73)],\n",
        "                         [(16,432),(679,499)]],\n",
        "               '41c25dca24':[[(6,18),(41,56)],[(385,444),(542,511)]],\n",
        "               'f32c7bd62b':[],\n",
        "               '5c1cd79e2e':[[(19,17),(99,92)]],\n",
        "               'f77a745ba2':[[(0,0),(82,69)],\n",
        "                         [(0,478),(690,511)]],\n",
        "               '7ab1f4b7ba':[],\n",
        "               'd722a3a383':[[(452,21),(500,68)]],\n",
        "               'cc87926397':[[(0,0),(31,23)],\n",
        "                         [(494,406),(716,508)],\n",
        "                         [(0,354),(135,379)]],\n",
        "               'f29baed81b':[[(20,20),(162,106)],\n",
        "                         [(556,432),(692,508)]],\n",
        "               '33f8a59619':[[(47,36),(145,132)],\n",
        "                         [(11,445),(162,511)]],\n",
        "               '08549eb98f':[[(435,14),(482,77)],\n",
        "                         [(14,437),(123,492)]],\n",
        "               'd8eff0e0bd':[[(0,11),(71,74)],\n",
        "                         [(0,443),(119,511)]],\n",
        "               'f73d0ac26a':[[(6,7),(150,49)],\n",
        "                         [(507,438),(594,485)]],\n",
        "               'e111fb569d':[[(0,26),(120,124)],\n",
        "                         [(0,432),(178,505)]],\n",
        "               'a45586af89':[[(0,0),(37,31)],\n",
        "                         [(4,483),(139,511)]],\n",
        "               'cb13cb5f85':[[(69,0),(105,28)],\n",
        "                         [(29,448),(195,490)]],\n",
        "               '0384c4e782':[[(25,23),(76,74)],\n",
        "                         [(13,455),(111,504)]],\n",
        "               'b1fc590830':[[(0,466),(686,511)]],\n",
        "               'fb1eb65fe5':[[(220,475),(300,525)]],\n",
        "               '03a1d18cc6':[[(9,8),(71,66)],\n",
        "                         [(0,451),(107,510)]],\n",
        "               'be5df16bd4':[],\n",
        "               '7e0c56c44b':[[(0,0),(74,62)],\n",
        "                         [(593,438),(737,497)]],\n",
        "               'ee161d333c':[[(55,387),(218,457)]],\n",
        "               'f447a36b0b':[[(24,12),(105,82)],\n",
        "                         [(27,412),(174,487)]],\n",
        "               '4d951b622f':[[(686,11),(746,64)],\n",
        "                         [(9,448),(142,502)]],\n",
        "               '3d103080e6':[[(0,0),(39,62)],\n",
        "                         [(477,432),(660,511)]],\n",
        "               '7988097925':[[(0,0),(75,110)],\n",
        "                         [(498,404),(744,511)]],\n",
        "               '226dc58f03':[[(8,6),(72,72)],\n",
        "                         [(475,490),(777,511)]],\n",
        "               'a4027c076c':[[(0,457),(684,511)]],\n",
        "               'b95828224e':[],\n",
        "               'f5235de51f':[[(0,0),(79,89)],\n",
        "                         [(521,416),(749,511)]],\n",
        "               '8dffc01cc2':[[(0,456),(674,511)]],\n",
        "               'bf6f1a320a':[[(517,432),(661,507)]],\n",
        "               '410284d28b':[[(34,39),(91,94)],\n",
        "                         [(314,463),(618,502)],\n",
        "                         [(34,475),(61,495)]],\n",
        "               'c61a50a061':[[(0,0),(87,66)],\n",
        "                         [(8,457),(228,508)]],\n",
        "               'e6bfc43298':[[(7,17),(202,77)],\n",
        "                         [(6,438),(165,498)]],\n",
        "               'e071eefebb':[[(0,0),(69,73)],\n",
        "                         [(35,453),(122,505)]],\n",
        "               'e493a405ad':[[(446,437),(485,485)],\n",
        "                         [(38,435),(105,475)]],\n",
        "               '893215335a':[[(17,13),(51,52)],\n",
        "                         [(519,442),(700,496)]],\n",
        "               '82e8007547':[[(0,483),(63,511)]],\n",
        "               '0fa38de2c7':[],\n",
        "               '62a54f335d':[[(23,18),(55,54)],\n",
        "                         [(17,437),(158,485)]],\n",
        "               'dc20484021':[[(12,31),(130,122)],\n",
        "                         [(570,418),(744,495)]],\n",
        "               'cd48200d46':[[(32,435),(166,496)]],\n",
        "               '691f7638fd':[[(11,9),(98,75)],\n",
        "                         [(529,425),(666,495)]],\n",
        "               'b657336f23':[[(363,456),(490,510)]],\n",
        "               'f0522acde8':[[(7,9),(66,66)],\n",
        "                         [(24,472),(172,494)]],\n",
        "               '345fb7ab68':[[(414,470),(678,511)],\n",
        "                         [(0,484),(328,511)]],\n",
        "               '1b3c6536a3':[[(11,9),(55,62)],\n",
        "                         [(0,457),(648,511)]],\n",
        "               'c1bfc4308b':[[(0,0),(80,80)],\n",
        "                         [(522,218),(610,261)],\n",
        "                         [(503,470),(610,511)]],\n",
        "               'c88cae3e95':[[(28,24),(95,72)],\n",
        "                         [(14,403),(141,479)]],\n",
        "               '8e1269eba3':[[(0,0),(75,98)],\n",
        "                         [(0,473),(679,511)]],\n",
        "               'd822219f76':[[(8,17),(117,104)],\n",
        "                         [(0,410),(167,511)]],\n",
        "               '8e68b05e0e':[[(575,0),(613,28)],\n",
        "                         [(2,470),(69,507)],\n",
        "                         [(516,459),(611,504)]],\n",
        "               '40afb05b44':[[(0,0),(18,24)],\n",
        "                         [(478,459),(622,485)],\n",
        "                         [(145,455),(383,488)]],\n",
        "               'e2d9d3d783':[[(14,9),(98,80)],\n",
        "                         [(6,441),(137,503)]],\n",
        "               '482193653a':[[(446,432),(572,487)]],\n",
        "               '85743c45d1':[[(21,22),(62,83)]],\n",
        "               'a9a628444d':[[(0,0),(95,95)],\n",
        "                         [(620,421),(740,480)]],\n",
        "               '41bb4d19a4':[[(0,0),(68,85)],\n",
        "                         [(456,446),(640,511)]],\n",
        "               '3b18ef9b73':[],\n",
        "               'fd1f58d925':[[(15,6),(77,58)],\n",
        "                         [(0,478),(641,511)],\n",
        "                         [(516,387),(598,458)]],\n",
        "               '07330c8ea1':[[(18,17),(81,74)],\n",
        "                         [(430,473),(511,525)],\n",
        "                         [(0,511),(511,525)]],\n",
        "               '1206963ed7':[[(3,25),(63,74)],\n",
        "                         [(387,471),(511,511)]],\n",
        "               '0113230c21':[[(10,12),(194,60)],\n",
        "                         [(515,472),(679,511)]],\n",
        "               'de1d3f1409':[[(15,34),(74,100)],\n",
        "                         [(275,411),(511,511)]],\n",
        "               'b3e7900e39':[[(384,461),(496,505)]],\n",
        "               '848b740671':[[(13,16),(45,57)],\n",
        "                         [(0,422),(156,511)]],\n",
        "               '934d76aaf2':[[(23,15),(57,50)],\n",
        "                         [(592,447),(700,505)]],\n",
        "               'c0fcfec15c':[[(3,6),(90,96)],\n",
        "                         [(12,411),(165,473)]],\n",
        "               'a1e18db01a':[[(8,28),(70,87)],\n",
        "                         [(401,427),(673,509)]],\n",
        "               '132788669c':[[(415,468),(489,504)]],\n",
        "               'bb3d8c922b':[],\n",
        "               '3ac1920a79':[[(0,0),(57,76)]],\n",
        "               '032bd4a6b1':[[(8,14),(56,72)],\n",
        "                         [(417,447),(597,511)]],\n",
        "               '854de5be19':[[(0,0),(52,57)],\n",
        "                         [(330,11),(489,56)],\n",
        "                         [(0,452),(151,515)]],\n",
        "               '7f1a5b0782':[[(0,0),(51,60)],\n",
        "                         [(559,423),(765,508)]],\n",
        "               'eed04b7d68':[[(15,27),(63,78)],\n",
        "                         [(0,475),(676,511)]],\n",
        "               '2bc87a8698':[[(0,478),(566,511)]],\n",
        "               '728f55aed4':[[(854,37),(985,164)],\n",
        "                         [(69,335),(342,468)]],\n",
        "               '72507b07bc':[[(25,17),(56,57)],\n",
        "                         [(26,442),(120,500)]],\n",
        "               'ec62d76c0a':[[(0,0),(55,45)],\n",
        "                         [(282,439),(488,503)]],\n",
        "               'd04d7ced9f':[[(19,16),(126,96)],\n",
        "                         [(0,480),(687,511)]],\n",
        "               '6d1574872b':[[(0,0),(43,79)],\n",
        "                         [(400,457),(547,511)]],\n",
        "               '6c52a10c75':[[(6,6),(70,70)],\n",
        "                         [(382,446),(502,499)]],\n",
        "               'ed3547f3da':[[(20,0),(95,95)],\n",
        "                         [(11,469),(100,519)]],\n",
        "               'ed120ef412':[[(0,0),(95,78)],\n",
        "                         [(530,458),(653,511)]],\n",
        "               'dcfbfa74e4':[[(0,0),(73,69)],\n",
        "                         [(587,436),(740,501)]],\n",
        "               '2807b90ea9':[[(23,16),(72,60)],\n",
        "                         [(0,433),(209,511)]],\n",
        "               'bf57480a38':[],\n",
        "               '404b0836f1':[[(2,15),(52,69)],\n",
        "                         [(96,447),(274,511)],\n",
        "                         [(8,480),(85,511)]],\n",
        "               '1700be1235':[[(0,0),(83,79)],\n",
        "                         [(279,0),(551,79)],\n",
        "                         [(243,437),(371,487)],\n",
        "                         [(130,437),(221,475)]],\n",
        "               '66a6ddb34e':[],\n",
        "               '17f1cecc7a':[[(0,0),(64,66)],\n",
        "                         [(415,0),(511,67)],\n",
        "                         [(348,436),(511,514)]],\n",
        "               '9928dad0ac':[[(8,12),(85,66)],\n",
        "                         [(5,457),(135,503)]],\n",
        "               '1bed563906':[[(0,0),(66,68)],\n",
        "                         [(4,455),(82,509)]],\n",
        "               '7d54a1c5b3':[[(0,533),(31,565)]],\n",
        "               '891f96f60d':[[(8,9),(101,66)],\n",
        "                         [(468,474),(760,506)]],\n",
        "               '1a29c5b84f':[[(11,466),(204,529)]],\n",
        "               'f485e2051b':[[(11,11),(66,72)],\n",
        "                         [(368,437),(514,511)]],\n",
        "               '50a4901372':[[(15,7),(78,60)],\n",
        "                         [(457,454),(714,501)]],\n",
        "               '724248a877':[[(16,0),(76,52)],\n",
        "                             [(0,482),(658,511)]],\n",
        "               'b4ab2ea927':[[(14,22),(91,91)],\n",
        "                         [(540,436),(691,505)]],\n",
        "               'dedf11eb6c':[[(23,29),(112,94)],\n",
        "                         [(511,414),(662,479)]],\n",
        "               '83fd5c9288':[[(0,6),(36,41)],\n",
        "                         [(10,470),(134,507)]],\n",
        "               'ea71fe4650':[[(0,0),(75,93)],\n",
        "                         [(0,476),(694,511)]],\n",
        "               'd54e727066':[[(0,444),(220,488)]],\n",
        "               '3692286ecf':[[(359,458),(667,504)]],\n",
        "               'a2883fc6eb':[[(3,404),(173,454)],\n",
        "                         [(3,463),(81,511)]],\n",
        "               '78c63579ba':[[(2,5),(51,54)],\n",
        "                         [(478,447),(589,508)]],\n",
        "               'cc9540d282':[[(0,25),(107,120)],\n",
        "                         [(437,418),(559,480)]],\n",
        "               '406975041a':[[(0,0),(57,54)],\n",
        "                         [(2,469),(91,511)]],\n",
        "               'd5e8abb61d':[[(6,23),(68,73)],\n",
        "                         [(30,443),(211,489)]],\n",
        "               '4545280c2b':[[(48,272),(210,333)]],\n",
        "               'dbbbce5020':[[(490,15),(540,70)],\n",
        "                         [(9,475),(97,507)]],\n",
        "               '3a0a2f5edf':[[(0,0),(67,93)],\n",
        "                         [(388,8),(502,59)],\n",
        "                         [(413,478),(504,499)]],\n",
        "               '8cee575b39':[[(13,23),(92,80)],\n",
        "                         [(260,400),(566,485)]],\n",
        "               '51f82fcb93':[[(0,0),(63,59)],\n",
        "                         [(3,429),(155,499)]],\n",
        "               '15a3ba1c51':[[(0,0),(63,65)],\n",
        "                         [(460,0),(740,155)],\n",
        "                         [(506,408),(735,508)]],\n",
        "               'e59a1a493e':[[(0,462),(710,511)]],\n",
        "               '5f42a8d4a9':[[(362,459),(486,505)]],\n",
        "               'e73bcbd77d':[[(0,0),(79,84)],\n",
        "                         [(520,425),(588,505)]],\n",
        "               '8f9db31734':[[(18,45),(58,93)],\n",
        "                         [(392,423),(557,498)]],\n",
        "               'd83980d44e':[[(5,0),(32,32)],\n",
        "                         [(0,476),(86,511)]],\n",
        "               '6a291d0bb4':[[(12,416),(161,486)]],\n",
        "               '95e201a1fe':[[(14,17),(89,77)]],\n",
        "               '2cd773d930':[[(0,0),(40,50)],\n",
        "                         [(6,466),(91,511)]],\n",
        "               'f84ce9e349':[[(0,0),(104,74)],\n",
        "                         [(386,435),(491,487)]],\n",
        "               'b5ba9813a4':[[(0,0),(32,33)],\n",
        "                         [(351,455),(500,500)]],\n",
        "               '516b2f0aa8':[[(459,11),(511,68)],\n",
        "                         [(9,467),(107,506)]],\n",
        "               '03cd0b2741':[[(12,34),(81,97)],\n",
        "                             [(4,468),(129,511)]],\n",
        "               '294049f7a8':[[(0,477),(683,511)]],\n",
        "               'fe83ead30b':[[(11,37),(69,95)],\n",
        "                         [(0,456),(149,511)]],\n",
        "               'e49ed62b0b':[],\n",
        "               'b616ffa779':[[(0,0),(69,77)],\n",
        "                         [(344,433),(505,509)]],\n",
        "               '1ff313314c':[],\n",
        "               'd3971746e0':[[(494,467),(600,503)]],\n",
        "               '4cd9e65aae':[[(4,4),(114,114)],\n",
        "                         [(543,470),(681,511)]],\n",
        "               '8c22d1acd5':[[(0,488),(592,511)]],\n",
        "               'a027b64ae9':[[(0,0),(67,63)],\n",
        "                         [(0,449),(115,511)]],\n",
        "               'b78f5fdc22':[],\n",
        "               'b78bdee5d3':[[(25,56),(62,90)],\n",
        "                         [(412,417),(554,489)]],\n",
        "               'c5c0825297':[[(0,0),(64,101)],\n",
        "                         [(463,0),(738,154)],\n",
        "                         [(497,414),(728,509)]],\n",
        "               '85dedcfd30':[[(0,0),(69,79)]],\n",
        "               'ef6bbe7612':[[(0,0),(96,80)],\n",
        "                         [(0,477),(729,511)]],\n",
        "               '70345dfad1':[[(22,36),(112,110)],\n",
        "                         [(651,469),(730,492)]],\n",
        "               'ffae2f363e':[[(28,17),(92,82)],\n",
        "                         [(15,445),(142,505)]],\n",
        "               '6752c27137':[],\n",
        "               '1ecd5c901c':[[(0,429),(582,511)]],\n",
        "               'f24b8804c9':[[(12,353),(129,505)]],\n",
        "               '85a5d4d8b9':[[(0,0),(72,97)],\n",
        "                         [(517,414),(736,511)]],\n",
        "               'e7187b91bf':[[(49,418),(166,475)]],\n",
        "               'bcab6454cf':[[(0,0),(113,23)],\n",
        "                         [(49,50),(142,82)],\n",
        "                         [(645,403),(731,493)]],\n",
        "               '9ef6e6b29d':[[(586,8),(620,50)]],\n",
        "               'c984680063':[[(0,0),(46,44)],\n",
        "                         [(0,483),(578,511)]],\n",
        "               '2387be5eaf':[[(0,10),(42,65)],\n",
        "                         [(670,10),(766,56)],\n",
        "                         [(635,442),(763,494)]],\n",
        "               '453123a947':[[(7,12),(84,79)],\n",
        "                         [(7,428),(255,509)]],\n",
        "               'f659af19da':[[(32,9),(73,71)],\n",
        "                         [(465,482),(758,508)]],\n",
        "               '5842aa1a32':[[(0,0),(95,95)]],\n",
        "               'ae001af29f':[[(708,0),(783,46)],\n",
        "                         [(622,441),(752,494)],\n",
        "                         [(0,488),(783,511)]],\n",
        "               'b23c478d84':[[(7,5),(47,31)],\n",
        "                         [(0,484),(532,511)]],\n",
        "               'fab3633337':[[(5,12),(81,87)],\n",
        "                         [(5,456),(132,510)]],\n",
        "               '6d0f510327':[[(0,0),(94,75)],\n",
        "                         [(376,448),(516,510)]],\n",
        "               '9c4905d020':[[(24,26),(81,107)],\n",
        "                         [(520,433),(641,505)]],\n",
        "               '62223ab32f':[[(4,4),(52,52)],\n",
        "                         [(523,474),(778,511)]],\n",
        "               '2fb8fe344c':[[(54,464),(222,511)]],\n",
        "               '9c29ca7edf':[],\n",
        "               '30da4fa249':[[(9,16),(52,58)],\n",
        "                         [(433,458),(528,503)]],\n",
        "               '5a6c23682b':[[(15,8),(124,68)],\n",
        "                         [(294,436),(521,499)]],\n",
        "               '55511ff512':[],\n",
        "               'ce39bb3c03':[],\n",
        "               'e6822fbe96':[],\n",
        "               'ce46819633':[],\n",
        "               '8bf3782470':[[(0,0),(91,84)],\n",
        "                         [(29,432),(200,508)]],\n",
        "               '7ecd3c2ab9':[[(0,8),(46,48)],\n",
        "                         [(410,465),(511,498)],\n",
        "                         [(0,513),(511,533)]],\n",
        "               'b19f6f3309':[[(10,14),(41,52)],\n",
        "                         [(1,472),(111,511)]],\n",
        "               '736f1a3eb2':[[(12,11),(63,46)],\n",
        "                         [(398,455),(534,511)]],\n",
        "               'ab82fe7827':[[(24,0),(129,86)],\n",
        "                         [(15,414),(182,483)]],\n",
        "               '2bf4aa0195':[],\n",
        "               'b0e3041a0d':[[(23,23),(72,72)],\n",
        "                         [(33,435),(680,498)]],\n",
        "               '4b07dbd803':[[(688,21),(767,79)],\n",
        "                         [(22,440),(222,493)]],\n",
        "               '2c6c108b4d':[[(6,24),(61,78)],\n",
        "                         [(413,440),(511,507)]],\n",
        "               'b8e9061c90':[[(0,0),(72,97)],\n",
        "                         [(12,462),(159,510)]],\n",
        "               '257e44104b':[[(24,6),(69,56)],\n",
        "                         [(38,483),(135,511)]],\n",
        "               '82c99bbb53':[[(9,23),(89,84)],\n",
        "                         [(0,443),(156,511)]]\n",
        "          }\n",
        "\n",
        "          # dic for recording corlor mode\n",
        "          self.color_mode_dic = self.patches_dic.copy()\n",
        "          for key in self.color_mode_dic:\n",
        "            self.color_mode_dic[key] = ['BinaryInterpolation_4Bonds' for i in range(len(self.color_mode_dic[key]))]\n",
        "\n",
        "          '''\n",
        "          for key in self.color_mode_dic:\n",
        "            self.color_mode_dic[key] = ['BinaryInterpolation_Sliding' for i in range(len(self.color_mode_dic[key]))]\n",
        "\n",
        "          for graph_name in ['dfeaa0d54b','cecb19b837','7a48f5b4d5','2fb8fe344c','3ac1920a79','7e85b64066','7ecd3c2ab9','7fabcfe23f','8e1269eba3','9c9d1af243','d933f429cc','c294fece54','c812c93fbd','cecb19b837','e4268d7502','ec62d76c0a','ec62d76c0a','1ecd5c901c','2cd773d930','3a0a2f5edf','3a9246e0ee','03cd0b2741','3d103080e6','4b07dbd803','4cd9e65aae','8bf3782470','9fa90de188','10d85f70bd','17f1cecc7a','33f8a59619','36de69b2c0','37ce835fb2','39b3a6db5e','39dda96b25','39e18a8f4c','41ac55ed12','41bb4d19a4','41c25dca24','48f3a148f6','50a4901372','51f82fcb93','58e8237dab','62e729e838','74ef917e76','78bda6c73b','78c63579ba','82c99bbb53','83afcabd95','85dedcfd30','95e201a1fe','96c772f28b','207bb15702','345fb7ab68','391ef00939','544b32e4cd','555eb05b4c','611b9564a4','627acee9c4','00655d9628','671b7c8831','691f7638fd','728f55aed4','736f1a3eb2','738c326d73','763c317d5d','802e607c7c','848b740671','854de5be19','854de5be19','891f96f60d','986f86c3b5','1700be1235','1736c60650','2181ebb330','2897b777fe','3027f9ffe7','5568be0f4c','5737e0c2c7','5842aa1a32','6360a823da','07330c8ea1','7701ac03c1','9928dad0ac','59424b060a','72507b07bc','74391d74bd','78834ed976','90615b870a','0113230c21','291898b7c6','453123a947','513158cf83','535134e785','707120d0f5','724248a877','4545280c2b','87429394b0','88808139d3','441414704c','893215335a','2556313831','a4b639ab1a','a492171fa8','aa309f6cec','aa968ec1b2','ab82fe7827','b0e3041a0d','b2f6f1bbf9','b3e7900e39','b4ab2ea927','b16f4f83a4','b19f6f3309','b23c478d84','b78bdee5d3','b85f5e414c','b616ffa779','bc1fe886af','bcab6454cf','bf6f1a320a','bf901d32a3','bfb2a25730','c0ba8a65a5','c0fcfec15c','c1bfc4308b','c6ea896731','c363833ad2','cc9540d282','cc87926397','cd48200d46','cecb19b837','cf6823322f','d3ed612a2d','d4b73860c3','d8eff0e0bd','d29d644539','d90c9c5b72','d722a3a383','d933f429cc','d6613c4ec5','d822219f76','dbbbce5020','dc25ff5266','dcfbfa74e4','de1d3f1409','dedf11eb6c','e1adc2ba25','e2d9d3d783','e6bfc43298','e8e39c913a','e49e69fd61','e59a1a493e','e071eefebb','e73bcbd77d','e111fb569d','e853d120fa','e4065c11e4','e4268d7502','eb67400438','ec2cc16d2a','ed120ef412','ed3547f3da','f7f9125fb0','f7fa7d1729','f24b8804c9','f28b702df5','f29baed81b','f77a745ba2','f84ce9e349','f659af19da','f664201c62','fab3633337','fc2851e6d3','fcebbb1fd9','ffae2f363e','fff35f80e1','19b0afdf81','1bf56ce626','5a65f9eba2','5e22377208','75e968af41','8e50748a7b','ac49e14130']:\n",
        "            self.color_mode_dic[graph_name] = ['BinaryInterpolation_4Bonds' for i in range(len(self.color_mode_dic[graph_name]))]\n",
        "\n",
        "          self.color_mode_dic['a492171fa8'] = ['BinaryInterpolation_Sliding'          for i in range(len(self.color_mode_dic['a492171fa8']))]\n",
        "          self.color_mode_dic['99eb4bad25'] = ['BinaryInterpolation_Sliding'          for i in range(len(self.color_mode_dic['7a48f5b4d5']))]\n",
        "          self.color_mode_dic['1b3c6536a3'] = ['BinaryInterpolation_Sliding', 'BinaryInterpolation_4Points']\n",
        "          '''\n",
        "\n",
        "     def fetch_patch(self, graph_name: str) -> list:\n",
        "          points_list = self.patches_dic[graph_name]\n",
        "          xy_limits_list = []\n",
        "\n",
        "          if points_list:\n",
        "               # fetch patch vertices\n",
        "               for points_set in points_list:\n",
        "                    # [(y1,x1),(y2,x2)]\n",
        "                    point_uperleft = points_set[0]\n",
        "                    point_botomright = points_set[1]\n",
        "\n",
        "                    # convert to column row for matrix\n",
        "                    y_min, x_min  = point_uperleft\n",
        "                    y_max, x_max  = point_botomright\n",
        "\n",
        "                    xy_limits = [x_min, x_max, y_min, y_max]\n",
        "                    xy_limits_list.append(xy_limits)\n",
        "\n",
        "               # fetch color mode\n",
        "               color_modes_list = self.color_mode_dic[graph_name]\n",
        "\n",
        "               return xy_limits_list, color_modes_list\n",
        "\n",
        "          else:\n",
        "               return [], []"
      ],
      "metadata": {
        "id": "Y4O3oqtQYwyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.4 Excution"
      ],
      "metadata": {
        "id": "YR_WBygyZI-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Firstly, you have image and mask from above code:\n",
        "- 1. make two sets of data:\n",
        "  - set A: patch (list of 4 coordinations for each sub-masks  which block each image)\n",
        "  - set B: mask with patch (Mask after filtering)\n",
        "\n",
        "- 2. Make the data runnable for the following code.\n",
        "  - For set A:\n",
        "  raw image + raw mask\n",
        "  (load the raw image and raw mask) \\\n",
        "  Write an algorithm to skip the masked region for training, validation and testing\n",
        "  - For set B:\n",
        "  Raw image + new mask\n",
        "  (Raw images data keep the same, replace all the raw masks into your new masked. No need to write anything)"
      ],
      "metadata": {
        "id": "LC5_m2cQ5P3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the data cleanning"
      ],
      "metadata": {
        "id": "DfiKq-xMDBTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialization\n",
        "## create work directory\n",
        "os.makedirs(Config.patched_image_folder, exist_ok=True)\n",
        "os.makedirs(Config.patched_mask_folder,  exist_ok=True)\n",
        "\n",
        "# initialize\n",
        "patch_databas     = Patch_Databas()\n",
        "graph_patches_dic = patch_databas.patches_dic\n",
        "graph_num         = len(patch_databas.patches_dic)\n",
        "index             = 1\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) # skip low contrast warning\n",
        "\n",
        "\n",
        "## excution\n",
        "# looping\n",
        "for graph_name in tqdm(graph_patches_dic):\n",
        "  try:\n",
        "    # fetch  image\n",
        "    imgae_exe  = Image(os.path.join(Config.image_folder,f'{graph_name}.png'))\n",
        "\n",
        "    # fetch  mask\n",
        "    mask_exe   = Mask(os.path.join(Config.mask_folder,f'{graph_name}.png'))\n",
        "\n",
        "    # reset patches\n",
        "    imgae_exe.patch_list = []\n",
        "\n",
        "    # fetch patch coordinate info\n",
        "    xy_limits_list, color_modes_list = patch_databas.fetch_patch(graph_name)\n",
        "\n",
        "    # create patches\n",
        "    patch_list = []\n",
        "    for patch_i_xy_limit, patch_color_mode in zip(xy_limits_list, color_modes_list):\n",
        "      patch_i = Patch(xy_limits=patch_i_xy_limit,    # input: coordinate\n",
        "                      color_mode=patch_color_mode)   # input: coloring mode\n",
        "      patch_list.append(patch_i)\n",
        "\n",
        "    # attach patch to Image\n",
        "    for patch_i in patch_list:\n",
        "      imgae_exe.attach_patch(patch_i)\n",
        "\n",
        "    # save image\n",
        "    io.imsave(os.path.join(Config.patched_image_folder,f'{graph_name}.png'), imgae_exe.data_with_patch)\n",
        "\n",
        "    # attach patch to Mask\n",
        "    for patch_i in patch_list:\n",
        "      patch_i.color_mode='GrayScale_background' # := \"filtered\", GrayScale_background := patch equals to background\n",
        "      mask_exe.attach_patch(patch_i)\n",
        "\n",
        "    # save mask\n",
        "    io.imsave(os.path.join(Config.patched_mask_folder,f'{graph_name}.png'), mask_exe.data_with_patch)\n",
        "\n",
        "    index += 1\n",
        "\n",
        "  except Exception as e:\n",
        "    # print error message\n",
        "    print(f'Error: graph name = {graph_name}, index = {index}/{graph_num}')\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPJqUzjGbNBc",
        "outputId": "db246f27-5e6a-42e0-8951-e703d55c4d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 126/465 [01:32<03:18,  1.71it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload package\n",
        "# for avoiding conflict between self-defined Image class and imported PIL-Image package in following steps\n",
        "yes | pip uninstall pillow\n",
        "yes | pip install pillow\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "MAzRR104kSaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Data preprocessing"
      ],
      "metadata": {
        "id": "sLG5TAHGj9yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes"
      ],
      "metadata": {
        "id": "OlCJ-Sxpg-uC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L2YQfD4DywM"
      },
      "outputs": [],
      "source": [
        "class ImageDataset:\n",
        "    def __init__(self, image_folder: str, mask_folder: str,transform =None,binary=False):\n",
        "        '''\n",
        "        Parameters:\n",
        "        image_folder: path to folder containing images\n",
        "        mask_folder: path to folder containing masks\n",
        "        '''\n",
        "        self.image_folder = image_folder\n",
        "        self.mask_folder  = mask_folder\n",
        "        self.image_files  = sorted(os.listdir(image_folder))\n",
        "        self.mask_files   = sorted(os.listdir(mask_folder))\n",
        "        self.transform    = transform\n",
        "        self.binary       = binary\n",
        "\n",
        "    def binary_transform(self, mask): # We only deal with one class\n",
        "        # Transform mask to 1,0. 1 for masked area, 0 for not\n",
        "        mask_array        = np.array(mask)\n",
        "        binary_mask       = (mask_array != 0).astype(np.uint8)\n",
        "        return Image.fromarray(binary_mask)\n",
        "\n",
        "    def __len__(self): # Dunder method, usage: len(obj)\n",
        "      return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx): # Dunder method, usage: obj[idx]\n",
        "      image_path = os.path.join(self.image_folder, self.image_files[idx])\n",
        "      mask_path  = os.path.join(self.mask_folder, self.mask_files[idx])\n",
        "      image      = Image.open(image_path).convert('L')\n",
        "      mask       = Image.open(mask_path).convert('L')\n",
        "      if self.binary:\n",
        "          mask = self.binary_transform(mask)\n",
        "      if self.transform:\n",
        "          image, mask = self.transform(image, mask)\n",
        "      return image, mask\n",
        "\n",
        "    def plot_figure(self, overlay=False, mask=True, binary=False, idx=None):\n",
        "        \"\"\"\n",
        "        overlay: overlapping the original and masked images\n",
        "        mask: display mask images\n",
        "        idx: index or list of indices of the images to plot\n",
        "        \"\"\"\n",
        "        if idx is None:\n",
        "            idx = [0]\n",
        "        elif isinstance(idx, int):\n",
        "            idx = [idx]\n",
        "        idx = [i for i in idx if i < len(self.image_files)]\n",
        "        total_cols = 3 if mask and overlay else 2 if mask else 1\n",
        "        total_rows = len(idx)\n",
        "        fig, axes = plt.subplots(total_rows, total_cols, figsize=(5 * total_cols, 5 * total_rows))\n",
        "\n",
        "        if total_rows == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, file_idx in enumerate(idx):\n",
        "            img, mask_img = self[file_idx]\n",
        "\n",
        "            # Convert tensors to numpy arrays\n",
        "            img_np = img.squeeze(0).numpy()  # Squeeze channel for grayscale\n",
        "            mask_np = mask_img.numpy()  # mask is already single-channel after transform\n",
        "\n",
        "            ax_row = axes[i] if total_rows > 1 else axes\n",
        "\n",
        "            # Display the image\n",
        "            ax_row[0].imshow(img_np, cmap='gray')\n",
        "            ax_row[0].set_title(f\"Image {self.image_files[file_idx]}\")\n",
        "            ax_row[0].axis('off')\n",
        "\n",
        "            if mask:\n",
        "                # Display the mask\n",
        "                ax_row[1].imshow(mask_np, cmap='inferno')\n",
        "                ax_row[1].set_title(f\"Mask {self.mask_files[file_idx]}\")\n",
        "                ax_row[1].axis('off')\n",
        "\n",
        "                if overlay:\n",
        "                    # Display the overlay\n",
        "                    ax_row[2].imshow(img_np, cmap='gray', alpha=0.9)\n",
        "                    ax_row[2].imshow(mask_np, cmap='inferno', alpha=0.4)\n",
        "                    ax_row[2].set_title(f\"Overlay {file_idx}\")\n",
        "                    ax_row[2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "class SegmentationTransform:\n",
        "    \"\"\" Class for transforming data:\n",
        "      1. Resizing\n",
        "      2. Converting to tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, resize=None):\n",
        "        self.resize         = resize\n",
        "        self.image_resize   = transforms.Resize(resize)\n",
        "        self.mask_resize    = transforms.Resize(resize, interpolation=Image.NEAREST)\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        image         = self.image_resize(image)\n",
        "        image_tensor  = torch.tensor(np.array(image), dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "        mask          = self.mask_resize(mask)\n",
        "        mask_array    = np.array(mask)\n",
        "        binary_mask   = (mask_array > 0).astype(np.float32)\n",
        "        mask_tensor   = torch.tensor(binary_mask)\n",
        "        return image_tensor, mask_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpandedRotationDataset(Dataset):\n",
        "    '''\n",
        "    '''\n",
        "    def __init__(self, base_dataset):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.base_len = len(self.base_dataset)\n",
        "        # Copy attributes if needed\n",
        "        self.image_files = self.base_dataset.image_files\n",
        "        self.mask_files = self.base_dataset.mask_files\n",
        "\n",
        "        # We now have 10 transformations per original image:\n",
        "        # 0-3: rotations (0°, 90°, 180°, 270°)\n",
        "        # 4-7: translations (random within ±50 pixels)\n",
        "        # 8-9: shears (e.g., +20° and -20°)\n",
        "        self.num_transforms = 10\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.base_len * self.num_transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        transform_group = idx % self.num_transforms\n",
        "        original_idx    = idx // self.num_transforms\n",
        "\n",
        "        image, mask     = self.base_dataset[original_idx]\n",
        "\n",
        "        img_np          = (image.squeeze().numpy() * 255).astype('uint8')\n",
        "        pil_image       = Image.fromarray(img_np, mode='L')\n",
        "        msk_np          = (mask.numpy() * 255).astype('uint8')\n",
        "        pil_mask        = Image.fromarray(msk_np, mode='L')\n",
        "\n",
        "        # Default transformation parameters\n",
        "        angle       = 0\n",
        "        translate   = (0, 0)\n",
        "        scale       = 1.0\n",
        "        shear       = 0\n",
        "\n",
        "        if transform_group in [0, 1, 2, 3]:\n",
        "            # Rotations\n",
        "            angle     = transform_group * 90\n",
        "            # We'll use PIL's rotate method for rotations\n",
        "            pil_image = pil_image.rotate(angle, expand=True)\n",
        "            pil_mask  = pil_mask.rotate(angle, expand=True)\n",
        "\n",
        "        elif transform_group in [4, 5, 6, 7]:\n",
        "            # Translations\n",
        "            # Use a seeded RNG for reproducibility\n",
        "            rng       = np.random.RandomState(original_idx * 100 + transform_group)\n",
        "            # Random x and y translations, each within ±50 pixels\n",
        "            tx        = rng.randint(-50, 51)\n",
        "            ty        = rng.randint(-50, 51)\n",
        "            translate = (tx, ty)\n",
        "\n",
        "            # We can use torchvision's affine for consistent transformations\n",
        "            pil_image   = TF.affine(pil_image, angle=0, translate=translate, scale=1.0, shear=0)\n",
        "            pil_mask    = TF.affine(pil_mask, angle=0, translate=translate, scale=1.0, shear=0)\n",
        "\n",
        "        elif transform_group in [8, 9]:\n",
        "            # Shears\n",
        "            # For example, +20° shear for transform_group = 8, and -20° shear for transform_group = 9\n",
        "            shear_angle   = 20 if transform_group == 8 else -20\n",
        "            # Apply shear with no rotation or translation\n",
        "            pil_image     = TF.affine(pil_image, angle=0, translate=(0,0), scale=1.0, shear=shear_angle)\n",
        "            pil_mask      = TF.affine(pil_mask, angle=0, translate=(0,0), scale=1.0, shear=shear_angle)\n",
        "\n",
        "        # After applying transformations, convert to tensors\n",
        "        if self.base_dataset.transform:\n",
        "            image_tensor, mask_tensor = self.base_dataset.transform(pil_image, pil_mask)\n",
        "        else:\n",
        "            image_tensor  = torch.tensor(np.array(pil_image), dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "            binary_mask   = (np.array(pil_mask) > 0).astype(np.float32)\n",
        "            mask_tensor   = torch.tensor(binary_mask)\n",
        "\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "    def plot_figure(self, overlay=False, mask=True, binary=False, idx=None):\n",
        "        if idx is None:\n",
        "            idx = [0]\n",
        "        elif isinstance(idx, int):\n",
        "            idx = [idx]\n",
        "\n",
        "        # Filter valid indices\n",
        "        idx = [i for i in idx if i < len(self)]\n",
        "\n",
        "        total_cols  = 3 if (mask and overlay) else (2 if mask else 1)\n",
        "        total_rows  = len(idx)\n",
        "        fig, axes   = plt.subplots(total_rows, total_cols, figsize=(5 * total_cols, 5 * total_rows))\n",
        "\n",
        "        if total_rows == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, file_idx in enumerate(idx):\n",
        "            img, mask_img = self[file_idx]\n",
        "            img_np = img.squeeze(0).numpy()\n",
        "            mask_np = mask_img.numpy()\n",
        "\n",
        "            transform_group = file_idx % self.num_transforms\n",
        "            original_idx = file_idx // self.num_transforms\n",
        "\n",
        "            # Determine transformation type for title\n",
        "            if transform_group in [0,1,2,3]:\n",
        "                angle = transform_group * 90\n",
        "                title_transform   = f\"Rot {angle}°\"\n",
        "            elif transform_group in [4,5,6,7]:\n",
        "                title_transform   = \"Translation\"\n",
        "            elif transform_group  == 8:\n",
        "                title_transform   = \"Shear +20°\"\n",
        "            else:\n",
        "                title_transform = \"Shear -20°\"\n",
        "\n",
        "            img_name = self.image_files[original_idx]\n",
        "            mask_name = self.mask_files[original_idx]\n",
        "\n",
        "            ax_row = axes[i] if total_rows > 1 else axes\n",
        "\n",
        "            # Show the image\n",
        "            ax_row[0].imshow(img_np, cmap='gray')\n",
        "            ax_row[0].set_title(f\"Image {img_name} ({title_transform})\")\n",
        "            ax_row[0].axis('off')\n",
        "\n",
        "            if mask:\n",
        "                ax_row[1].imshow(mask_np, cmap='inferno')\n",
        "                ax_row[1].set_title(f\"Mask {mask_name} ({title_transform})\")\n",
        "                ax_row[1].axis('off')\n",
        "\n",
        "                if overlay:\n",
        "                    ax_row[2].imshow(img_np, cmap='gray', alpha=0.9)\n",
        "                    ax_row[2].imshow(mask_np, cmap='inferno', alpha=0.4)\n",
        "                    ax_row[2].set_title(f\"Overlay {file_idx}\")\n",
        "                    ax_row[2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "iSt7_iEBHSdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Data Loader"
      ],
      "metadata": {
        "id": "z_MIvW2CBQIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign image and mask folder. Test the imagedata set class:\n",
        "transform = SegmentationTransform(resize = config.resize) # can add resize if needed but not recommended\n",
        "\n",
        "# Original data\n",
        "# select prestine data\n",
        "'''\n",
        "origianl_dataset      = ImageDataset(config.image_folder, # path to image folder\n",
        "                                     config.mask_folder,  # path to mask folder\n",
        "                                     transform=transform,\n",
        "                                     binary=True)\n",
        "'''\n",
        "# select patched data\n",
        "origianl_dataset      = ImageDataset(config.patched_image_folder, # path to image folder\n",
        "                                     config.patched_mask_folder,  # path to mask folder\n",
        "                                     transform=transform,\n",
        "                                     binary=True)\n",
        "\n",
        "\n",
        "original_dataset_size = len(origianl_dataset)\n",
        "train_size_original   = int(0.6 * original_dataset_size)\n",
        "val_size_original     = int(0.2 * original_dataset_size)\n",
        "test_size_original    = original_dataset_size - train_size_original - val_size_original\n",
        "\n",
        "# Original\n",
        "train_dataset_original, val_dataset_original, test_dataset_original = random_split(\n",
        "    origianl_dataset,[train_size_original, val_size_original, test_size_original], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# optional augmented dataset\n",
        "dataset = ExpandedRotationDataset(origianl_dataset)\n",
        "dataset_size = len(dataset)\n",
        "\"\"\"\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.6 * dataset_size)\n",
        "val_size = int(0.2 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\"\"\"\n",
        "train_original_indices = train_dataset_original.indices\n",
        "val_original_indices = val_dataset_original.indices\n",
        "test_original_indices = test_dataset_original.indices\n",
        "train_indices = []\n",
        "for i in train_original_indices:\n",
        "    for j in range(10):\n",
        "        train_indices.append(i * 10 + j)\n",
        "\n",
        "val_indices = []\n",
        "for i in val_original_indices:\n",
        "    for j in range(10):\n",
        "        val_indices.append(i * 10 + j)\n",
        "\n",
        "test_indices = []\n",
        "for i in test_original_indices:\n",
        "    for j in range(10):\n",
        "        test_indices.append(i * 10 + j)\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Data loaders\n",
        "\"\"\"\n",
        "We can set:\n",
        "batch size;\n",
        "shuffle the data or not\n",
        "num_workers: higher value speeding up the loading process.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "              batch_size=config.batch_size,\n",
        "              shuffle=True,\n",
        "              num_workers=4\n",
        "              )\n",
        "val_loader = DataLoader(val_dataset,\n",
        "              batch_size=config.batch_size, shuffle=False, num_workers=4\n",
        "              )\n",
        "test_loader = DataLoader(test_dataset,\n",
        "              batch_size=config.batch_size, shuffle=False, num_workers=4\n",
        "              )\n",
        "train_loader_original = DataLoader(train_dataset_original,\n",
        "              batch_size=config.batch_size,\n",
        "              shuffle=True,\n",
        "              num_workers=4\n",
        "              )\n",
        "val_loader_original = DataLoader(val_dataset_original,\n",
        "              batch_size=config.batch_size, shuffle=False, num_workers=4\n",
        "              )\n",
        "test_loader_original = DataLoader(test_dataset_original,\n",
        "              batch_size=config.batch_size, shuffle=False, num_workers=4\n",
        "              )\n",
        "# dataset.plot_figure(overlay=True, mask=True, binary=True,idx=[10,11,12,13,14,15,16,17,20, 21, 22, 23, 24, 25])"
      ],
      "metadata": {
        "id": "9CRRt2YrkBrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print all the indices to check if they are correct!!!\n",
        "\n",
        "# Save original split indices\n",
        "pd.DataFrame(train_original_indices).to_csv('train_original_indices.csv', index=False, header=[\"index\"])\n",
        "pd.DataFrame(val_original_indices).to_csv('val_original_indices.csv', index=False, header=[\"index\"])\n",
        "pd.DataFrame(test_original_indices).to_csv('test_original_indices.csv', index=False, header=[\"index\"])\n",
        "\n",
        "# Save augmented (rotated) dataset indices\n",
        "pd.DataFrame(train_indices).to_csv('train_augmented_indices.csv', index=False, header=[\"index\"])\n",
        "pd.DataFrame(val_indices).to_csv('val_augmented_indices.csv', index=False, header=[\"index\"])\n",
        "pd.DataFrame(test_indices).to_csv('test_augmented_indices.csv', index=False, header=[\"index\"])"
      ],
      "metadata": {
        "id": "6JbwpRG6HeAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_train_idxs = train_indices[:10]\n",
        "dataset.plot_figure(idx=selected_train_idxs, overlay=True, mask=True, binary=True)"
      ],
      "metadata": {
        "id": "zaa9GrfcId9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_img1,example_mask1 = dataset[1]\n",
        "\n",
        "example_mask1_np = np.array(example_mask1)\n",
        "np.max(example_mask1_np)"
      ],
      "metadata": {
        "id": "LXPMEldDYcxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.plot_figure(overlay=True, mask=True, binary=True,idx=[277,278,279,289,400])"
      ],
      "metadata": {
        "id": "0a-jRCQx1HZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Model preparation\n"
      ],
      "metadata": {
        "id": "d71hTqyUCaG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 VGG-16\n"
      ],
      "metadata": {
        "id": "sHzsuPmzCjcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Architectures of VGG-16:\n",
        "\"\"\"\n",
        "# input: (1,1,512,512)\n",
        "(0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(1): ReLU(inplace=True)\n",
        "(2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(3): ReLU(inplace=True)\n",
        "(4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)# Pool1(1,64,256,256)\n",
        "(5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(6): ReLU(inplace=True)\n",
        "(7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(8): ReLU(inplace=True)\n",
        "(9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)#Pool2 (1,128,128,128)\n",
        "(10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(11): ReLU(inplace=True)\n",
        "(12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(13): ReLU(inplace=True)\n",
        "(14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(15): ReLU(inplace=True)\n",
        "(16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) #Pool3 (1,256,64,64)\n",
        "(17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(18): ReLU(inplace=True)\n",
        "(19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(20): ReLU(inplace=True)\n",
        "(21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(22): ReLU(inplace=True)\n",
        "(23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) #Pool4(1,512,32,32)\n",
        "(24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(25): ReLU(inplace=True)\n",
        "(26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(27): ReLU(inplace=True)\n",
        "(28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "(29): ReLU(inplace=True)\n",
        "(30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) #Pool5(1,512,16,16)\n",
        ")\n",
        "\"\"\"\n",
        "#FCN only, discard FCL\n",
        "class FCN8s_VGG16(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FCN8s_VGG16, self).__init__()\n",
        "        vgg = models.vgg16(pretrained=True) # VGG from pytorch\n",
        "\n",
        "        vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1) #remember we only have one channel not RGB, see [0] layer above\n",
        "        #features = list(vgg.features.children())\n",
        "        self.features = nn.Sequential(*list(vgg.features.children()))\n",
        "\n",
        "        self.score_pool3 = nn.Conv2d(256, num_classes, kernel_size=1) #convert 256 to number_class=1,dimension reduction\n",
        "        self.score_pool4 = nn.Conv2d(512, num_classes, kernel_size=1) #convert 512 to 1\n",
        "        self.score_fr = nn.Conv2d(512, num_classes, kernel_size=1)  #convert 512 to 1\n",
        "\n",
        "        self.upscore2 = nn.ConvTranspose2d(num_classes, num_classes, 2, stride=2) #upsample to original data size *2\n",
        "        self.upscore_pool4 = nn.ConvTranspose2d(num_classes, num_classes, 2, stride=2)#upsample to original data size *2\n",
        "        self.upscore8 = nn.ConvTranspose2d(num_classes, num_classes, 8, stride=8)#upsample to original data size *8\n",
        "\n",
        "    def forward(self, x):\n",
        "        pool3 = self.features[:17](x)\n",
        "        pool4 = self.features[17:24](pool3)\n",
        "        pool5 = self.features[24:](pool4)\n",
        "        score_fr = self.score_fr(pool5)#Reduces the depth of pool5 features to the number of classes using self.score_fr. (batch, num_classes, H/32, W/32)\n",
        "\n",
        "        upscore2 = self.upscore2(score_fr) #upsample by 2 because of pool4 1,1,16,16 ->1,1,32,32 senmatic part (H-1)* stride + kernel_size - 2*[padding=0] = (16-1) *2 +2 =32\n",
        "        score_pool4 = self.score_pool4(pool4) ## Shape: (batch, num_classes, H/16, W/16) spatial part\n",
        "        fuse_pool4 = upscore2 + score_pool4 #semantic + spatial\n",
        "\n",
        "        upscore_pool4 = self.upscore_pool4(fuse_pool4) #Shape: (batch, num_classes, H/8, W/8)\n",
        "        score_pool3 = self.score_pool3(pool3)\n",
        "        fuse_pool3 = upscore_pool4 + score_pool3\n",
        "\n",
        "        out = self.upscore8(fuse_pool3)#1,1,512,512 (64-1) *8 +8 = 512\n",
        "        return out\n",
        "\n",
        "model = FCN8s_VGG16(num_classes=config.num_classes)\n",
        "model = model.to(config.device)\n"
      ],
      "metadata": {
        "id": "c8L6xSVMCg8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 ResNet-50"
      ],
      "metadata": {
        "id": "7JtSs4QeT4Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.segmentation import fcn_resnet50\n",
        "\n",
        "model = fcn_resnet50(pretrained=True)\n",
        "model.classifier[4] = nn.Conv2d(512, config.num_classes, kernel_size=1)\n",
        "model.aux_classifier[4] = nn.Conv2d(256, config.num_classes, kernel_size=1)\n",
        "model = model.to(config.device)"
      ],
      "metadata": {
        "id": "Zu_Z7ndhUJF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 U-Net"
      ],
      "metadata": {
        "id": "SEywEIm2Uhxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "Unet_model = smp.Unet(\n",
        "    encoder_name='resnet34',                  # Choose encoder, e.g., resnet34\n",
        "    encoder_weights='imagenet',               # Use pretrained weights from ImageNet\n",
        "    in_channels=1,                            # Input channels (RGB images)\n",
        "    classes=config.num_classes,               # Number of output classes\n",
        "    encoder_depth=5,                          # Tunable depth\n",
        "    decoder_channels=[256, 128, 64, 32, 16],  # Tunable decoder channels\n",
        ")\n",
        "Unet_model = Unet_model.to(config.device)\n"
      ],
      "metadata": {
        "id": "YV33qilkUkwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 DeepLabv3+\n"
      ],
      "metadata": {
        "id": "6I5iH5GKUt-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "\n",
        "model = deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4] = nn.Conv2d(256, config.num_classes, kernel_size=1)\n",
        "model = model.to(config.device)\n"
      ],
      "metadata": {
        "id": "J1-EvqcnU0A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Mask R CNN"
      ],
      "metadata": {
        "id": "kJon8MOgU1vo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "znivOtCPJXI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.6 Res50-YNET"
      ],
      "metadata": {
        "id": "VEzGU_jhrGeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34Encoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ResNet34Encoder, self).__init__()\n",
        "        resnet = models.resnet34(pretrained=pretrained)\n",
        "        # Modify the first conv layer for 1-channel input\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Initialize weights\n",
        "        self.conv1.weight.data = resnet.conv1.weight.data.mean(dim=1, keepdim=True)\n",
        "        self.bn1 = resnet.bn1\n",
        "        self.relu = resnet.relu\n",
        "        self.maxpool = resnet.maxpool\n",
        "        # ResNet layers\n",
        "        self.layer1 = resnet.layer1  # Output: [B, 64, H/4, W/4]\n",
        "        self.layer2 = resnet.layer2  # Output: [B, 128, H/8, W/8]\n",
        "        self.layer3 = resnet.layer3  # Output: [B, 256, H/16, W/16]\n",
        "        self.layer4 = resnet.layer4  # Output: [B, 512, H/32, W/32]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.conv1(x)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "        x1 = self.maxpool(x0)\n",
        "        x2 = self.layer1(x1)\n",
        "        x3 = self.layer2(x2)\n",
        "        x4 = self.layer3(x3)\n",
        "        x5 = self.layer4(x4)\n",
        "        return x5, [x0, x1, x2, x3, x4]\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, num_layers=3):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(in_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ))\n",
        "        self.bottleneck = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bottleneck(x)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec1 = self._block(256 + 256, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self._block(128 + 128, 128)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec3 = self._block(64 + 64, 64)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "        self.dec4 = self._block(64 + 64, 64)\n",
        "\n",
        "        self.up5 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec5 = self._block(32 + 64, 32)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "\n",
        "    def _block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SEBlock(out_channels)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        x0, x1, x2, x3, x4 = encoder_features\n",
        "        # Up 1\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, x4], dim=1)\n",
        "        x = self.dec1(x)\n",
        "        # Up 2\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.dec2(x)\n",
        "        # Up 3\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.dec3(x)\n",
        "        # Up 4\n",
        "        x = self.up4(x)\n",
        "        x1_up = F.interpolate(x1, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x1_up], dim=1)\n",
        "        x = self.dec4(x)\n",
        "        # Up 5\n",
        "        x = self.up5(x)\n",
        "        x0_up = F.interpolate(x0, scale_factor=2, mode='bilinear', align_corners=False)  # Corrected here\n",
        "        x = torch.cat([x, x0_up], dim=1)\n",
        "        x = self.dec5(x)\n",
        "        # Final output\n",
        "        output = self.final_conv(x)\n",
        "        return output\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, _, _ = x.size()\n",
        "        # Squeeze\n",
        "        y = F.adaptive_avg_pool2d(x, 1).view(batch, channels)\n",
        "        # Excitation\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.sigmoid(y).view(batch, channels, 1, 1)\n",
        "        return x * y\n",
        "class YNetResNet34_Attention(nn.Module):\n",
        "    def __init__(self, num_classes=1, pretrained=True):\n",
        "        super(YNetResNet34_Attention, self).__init__()\n",
        "        self.encoder = ResNet34Encoder(pretrained=pretrained)\n",
        "        self.bottleneck1 = Bottleneck(512, num_layers=5)\n",
        "        self.bottleneck2 = Bottleneck(512, num_layers=5)\n",
        "        self.combine_conv = nn.Sequential(\n",
        "            nn.Conv2d(512 * 2, 512, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SEBlock(512)\n",
        "        )\n",
        "        self.decoder = Decoder(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc, encoder_features = self.encoder(x)\n",
        "        x_bottleneck1 = self.bottleneck1(x_enc)\n",
        "        x_bottleneck2 = self.bottleneck2(x_enc)\n",
        "        x_combined = torch.cat([x_bottleneck1, x_bottleneck2], dim=1)\n",
        "        x_combined = self.combine_conv(x_combined)\n",
        "        output = self.decoder(x_combined, encoder_features)\n",
        "        return output\n",
        "\n",
        "model = YNetResNet34_Attention(num_classes=1, pretrained=True)\n",
        "model = model.to(config.device)"
      ],
      "metadata": {
        "id": "QoeEUDxLrOYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.7 Attention-Res-YNet"
      ],
      "metadata": {
        "id": "wF7o-8XZt6P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        reduced_channels = max(1, in_channels // reduction)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, reduced_channels, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(reduced_channels, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return x * self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        out = self.conv(x_cat)\n",
        "        return x * self.sigmoid(out)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "# Encoder with ResNet34 backbone\n",
        "class ResNet34Encoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ResNet34Encoder, self).__init__()\n",
        "        resnet = models.resnet34(pretrained=pretrained)\n",
        "        # Modify the first conv layer for 1-channel input\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.conv1.weight.data = resnet.conv1.weight.data.mean(dim=1, keepdim=True)\n",
        "        self.bn1 = resnet.bn1\n",
        "        self.relu = resnet.relu\n",
        "        self.maxpool = resnet.maxpool\n",
        "        # ResNet layers\n",
        "        self.layer1 = resnet.layer1  # Output: [B, 64, H/4, W/4]\n",
        "        self.layer2 = resnet.layer2  # Output: [B, 128, H/8, W/8]\n",
        "        self.layer3 = resnet.layer3  # Output: [B, 256, H/16, W/16]\n",
        "        self.layer4 = resnet.layer4  # Output: [B, 512, H/32, W/32]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.conv1(x)   # [B, 64, H/2, W/2]\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "        x1 = self.maxpool(x0)  # [B, 64, H/4, W/4]\n",
        "        x2 = self.layer1(x1)   # [B, 64, H/4, W/4]\n",
        "        x3 = self.layer2(x2)   # [B, 128, H/8, W/8]\n",
        "        x4 = self.layer3(x3)   # [B, 256, H/16, W/16]\n",
        "        x5 = self.layer4(x4)   # [B, 512, H/32, W/32]\n",
        "        return x5, [x0, x1, x2, x3, x4]\n",
        "# Bottleneck with CBAM\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, num_layers=4):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(in_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                CBAM(in_channels)\n",
        "            ))\n",
        "        self.bottleneck = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bottleneck(x)\n",
        "# Decoder with CBAM\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec1 = self._block(256 + 256, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self._block(128 + 128, 128)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec3 = self._block(64 + 64, 64)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "        self.dec4 = self._block(64 + 64, 64)\n",
        "\n",
        "        self.up5 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec5 = self._block(32 + 64, 32)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "\n",
        "    def _block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            CBAM(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        x0, x1, x2, x3, x4 = encoder_features\n",
        "        # Up 1\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, x4], dim=1)\n",
        "        x = self.dec1(x)\n",
        "        # Up 2\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.dec2(x)\n",
        "        # Up 3\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.dec3(x)\n",
        "        # Up 4\n",
        "        x = self.up4(x)\n",
        "        x1_up = F.interpolate(x1, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x1_up], dim=1)\n",
        "        x = self.dec4(x)\n",
        "        # Up 5\n",
        "        x = self.up5(x)\n",
        "        x0_up = F.interpolate(x0, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x0_up], dim=1)\n",
        "        x = self.dec5(x)\n",
        "        # Final output\n",
        "        output = self.final_conv(x)\n",
        "        return output\n",
        "# Complete YNet with ResNet34 and CBAM\n",
        "class YNetResNet34_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=1, pretrained=True):\n",
        "        super(YNetResNet34_CBAM, self).__init__()\n",
        "        self.encoder = ResNet34Encoder(pretrained=pretrained)\n",
        "        self.bottleneck1 = Bottleneck(512, num_layers=4)\n",
        "        self.bottleneck2 = Bottleneck(512, num_layers=4)\n",
        "        self.combine_conv = nn.Sequential(\n",
        "            nn.Conv2d(512 * 2, 512, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            CBAM(512)\n",
        "        )\n",
        "        self.decoder = Decoder(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc, encoder_features = self.encoder(x)\n",
        "        x_bottleneck1 = self.bottleneck1(x_enc)\n",
        "        x_bottleneck2 = self.bottleneck2(x_enc)\n",
        "        x_combined = torch.cat([x_bottleneck1, x_bottleneck2], dim=1)\n",
        "        x_combined = self.combine_conv(x_combined)\n",
        "        output = self.decoder(x_combined, encoder_features)\n",
        "        return output\n",
        "model = YNetResNet34_CBAM(num_classes=config.num_classes, pretrained=True)\n",
        "model = model.to(config.device)"
      ],
      "metadata": {
        "id": "oqaMOtXEuG__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.8 Attention-YNet"
      ],
      "metadata": {
        "id": "sT74pJUhI994"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        reduced_channels = max(1, in_channels // reduction)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, reduced_channels, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(reduced_channels, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return x * self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        out = self.conv(x_cat)\n",
        "        return x * self.sigmoid(out)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "class ResNet34Encoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ResNet34Encoder, self).__init__()\n",
        "        resnet = models.resnet34(pretrained=pretrained)\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.conv1.weight.data = resnet.conv1.weight.data.mean(dim=1, keepdim=True)\n",
        "        self.bn1 = resnet.bn1\n",
        "        self.relu = resnet.relu\n",
        "        self.maxpool = resnet.maxpool\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.conv1(x)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "\n",
        "        x1 = self.maxpool(x0)\n",
        "        x2 = self.layer1(x1)\n",
        "        x3 = self.layer2(x2)\n",
        "        x4 = self.layer3(x3)\n",
        "        x_enc = self.layer4(x4)\n",
        "        return x_enc, [x0, x1, x2, x3, x4]\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, num_layers=4):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(in_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                CBAM(in_channels)\n",
        "            ))\n",
        "        self.bottleneck = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bottleneck(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec1 = self._block(256 + 256, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self._block(128 + 128, 128)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec3 = self._block(64 + 64, 64)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "        self.dec4 = self._block(64 + 64, 64)\n",
        "\n",
        "        self.up5 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec5 = self._block(32 + 64, 32)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "\n",
        "    def _block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            CBAM(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        x0, x1, x2, x3, x4 = encoder_features\n",
        "\n",
        "        # Up 1\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, x4], dim=1)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        # Up 2\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.dec2(x)\n",
        "\n",
        "        # Up 3\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        # Up 4\n",
        "        x = self.up4(x)\n",
        "        x1_up = F.interpolate(x1, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x1_up], dim=1)\n",
        "        x = self.dec4(x)\n",
        "\n",
        "        # Up 5\n",
        "        x = self.up5(x)\n",
        "        x0_up = F.interpolate(x0, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x0_up], dim=1)\n",
        "        x = self.dec5(x)\n",
        "\n",
        "        output = self.final_conv(x)\n",
        "        return output\n",
        "\n",
        "class YNetResNet34_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=1, pretrained=True):\n",
        "        super(YNetResNet34_CBAM, self).__init__()\n",
        "        self.encoder = ResNet34Encoder(pretrained=pretrained)\n",
        "\n",
        "        # After encoder:\n",
        "        # x_enc: 512 channels\n",
        "        # x4: 256 channels, x3:128 channels, x2:64 channels, x1:64 channels\n",
        "        # Total = 1024 channels\n",
        "        self.bottleneck_reduce = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.bottleneck1 = Bottleneck(512, num_layers=4)\n",
        "        self.bottleneck2 = Bottleneck(512, num_layers=4)\n",
        "\n",
        "        self.combine_conv = nn.Sequential(\n",
        "            nn.Conv2d(512 * 2, 512, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            CBAM(512)\n",
        "        )\n",
        "        self.decoder = Decoder(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc, encoder_features = self.encoder(x)\n",
        "        x0, x1, x2, x3, x4 = encoder_features\n",
        "\n",
        "        # Match spatial sizes to x_enc\n",
        "        h, w = x_enc.size(2), x_enc.size(3)\n",
        "        x4 = F.interpolate(x4, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        x3 = F.interpolate(x3, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        x2 = F.interpolate(x2, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        x1 = F.interpolate(x1, size=(h, w), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Now concatenate\n",
        "        x_combined_features = torch.cat([x_enc, x4, x3, x2, x1], dim=1)\n",
        "\n",
        "        x_bottleneck_input = self.bottleneck_reduce(x_combined_features)\n",
        "        x_bottleneck1 = self.bottleneck1(x_bottleneck_input)\n",
        "        x_bottleneck2 = self.bottleneck2(x_bottleneck_input)\n",
        "\n",
        "        x_combined = torch.cat([x_bottleneck1, x_bottleneck2], dim=1)\n",
        "        x_combined = self.combine_conv(x_combined)\n",
        "\n",
        "        # Pass combined to decoder along with original encoder features\n",
        "        output = self.decoder(x_combined, encoder_features)\n",
        "        return output\n",
        "model = YNetResNet34_CBAM(num_classes=config.num_classes, pretrained=True)\n",
        "model = model.to(config.device)"
      ],
      "metadata": {
        "id": "nBgreq0iJCQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.9 Swin-Unet (Binjie)"
      ],
      "metadata": {
        "id": "OH8HTb9-ktVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the code is executable"
      ],
      "metadata": {
        "id": "u2m3SdgKk5YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.10 R-CNN (Jingli)"
      ],
      "metadata": {
        "id": "WOrT-mealFLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply object detection"
      ],
      "metadata": {
        "id": "WBUtrKWYlJOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.11 SAM (Jingli)"
      ],
      "metadata": {
        "id": "YjGBlDAulPo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if SAM can have better prediction"
      ],
      "metadata": {
        "id": "o_JBNgDUla21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Training and testing"
      ],
      "metadata": {
        "id": "3aa-bSvkVGAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate customized\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "def custom_lr(epoch):\n",
        "    if epoch < 10:\n",
        "        return 1.0  # Keep the initial LR\n",
        "    #elif 10 <= epoch < 20:\n",
        "        #return 0.3\n",
        "    elif 10 <= epoch < 15:\n",
        "        return 0.5\n",
        "    elif 15 <= epoch < 20:\n",
        "        return 0.2\n",
        "    #elif 50 <= epoch < 70:\n",
        "        #return 0.2\n",
        "    else:\n",
        "        return 0.1\n",
        "\n",
        "## reference https://www.kaggle.com/code/banddaniel/particle-segmentation-deeplabv3-test-dice-0-89\n",
        "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = y_true.float().view(-1)\n",
        "    y_pred_f = y_pred.float().view(-1)\n",
        "    intersection = (y_true_f * y_pred_f).sum()\n",
        "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def jaccard_index(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = y_true.float().view(-1)\n",
        "    y_pred_f = y_pred.float().view(-1)\n",
        "    intersection = (y_true_f * y_pred_f).sum()\n",
        "    union = y_true_f.sum() + y_pred_f.sum() - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "from segmentation_models_pytorch.losses import FocalLoss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, weight_bce=1.0, weight_dice=1.0, weight_focal=1.0):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.dice_weight = weight_dice\n",
        "        self.bce_weight = weight_bce\n",
        "        self.focal_weight = weight_focal\n",
        "        self.focal_loss = FocalLoss(mode='binary')\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        # Ensure outputs and targets have the same shape\n",
        "        assert outputs.shape == targets.shape, f\"Shape mismatch: outputs {outputs.shape}, targets {targets.shape}\"\n",
        "\n",
        "        bce_loss = self.bce(outputs, targets)\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        dice_loss = dice_coef_loss(targets, probs)\n",
        "        focal_loss = self.focal_loss(outputs, targets)\n",
        "\n",
        "        total_loss = (self.bce_weight * bce_loss +\n",
        "                      self.dice_weight * dice_loss +\n",
        "                      self.focal_weight * focal_loss)\n",
        "        return total_loss\n",
        "\n",
        "# Instantiate the criterion\n",
        "criterion = CombinedLoss(weight_bce=0.2, weight_dice=1.0, weight_focal=5.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=custom_lr)"
      ],
      "metadata": {
        "id": "lI7PDb24VNlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Training"
      ],
      "metadata": {
        "id": "qcggfo5IVUAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "num_epochs = config.num_epochs\n",
        "train_loss_data = []\n",
        "val_loss_data = []\n",
        "precision_data = []\n",
        "recall_data = []\n",
        "accuracy_data = []\n",
        "iou_data = []\n",
        "f1_data = []\n",
        "dice_coeff_data = []\n",
        "rand_error_data = []\n",
        "pixel_error_data = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images = images.to(config.device)\n",
        "        masks = masks.to(config.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        outputs = outputs.squeeze(1)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    train_loss_data.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    total_pixels = 0\n",
        "    dice_coeff_sum = 0.0\n",
        "    iou_sum = 0.0\n",
        "    rand_error_sum = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images = images.to(config.device)\n",
        "            masks = masks.to(config.device)\n",
        "            outputs = model(images)\n",
        "            if isinstance(outputs, dict):\n",
        "                outputs = outputs['out']\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            # Threshold probabilities to get binary predictions\n",
        "            preds = (probs > 0.5).float()\n",
        "            # Flatten tensors to calculate metrics\n",
        "            preds_flat = preds.view(-1)\n",
        "            masks_flat = masks.view(-1)\n",
        "            # Calculate TP, FP, TN, FN\n",
        "            TP += ((preds_flat == 1) & (masks_flat == 1)).sum().item()\n",
        "            FP += ((preds_flat == 1) & (masks_flat == 0)).sum().item()\n",
        "            TN += ((preds_flat == 0) & (masks_flat == 0)).sum().item()\n",
        "            FN += ((preds_flat == 0) & (masks_flat == 1)).sum().item()\n",
        "            total_pixels += masks_flat.numel()\n",
        "            # Use the dice_coef and jaccard_index functions\n",
        "            dice_coeff = dice_coef(masks_flat, preds_flat).item()\n",
        "            dice_coeff_sum += dice_coeff\n",
        "            iou = jaccard_index(masks_flat, preds_flat).item()\n",
        "            iou_sum += iou\n",
        "            # Rand Error\n",
        "            preds_np = preds_flat.cpu().numpy()\n",
        "            masks_np = masks_flat.cpu().numpy()\n",
        "            rand_error = 1 - adjusted_rand_score(masks_np, preds_np)\n",
        "            rand_error_sum += rand_error\n",
        "\n",
        "    val_loss /= len(val_dataset)\n",
        "    val_loss_data.append(val_loss)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = TP / (TP + FP + 1e-6)\n",
        "    recall = TP / (TP + FN + 1e-6)\n",
        "    accuracy = (TP + TN) / (total_pixels + 1e-6)\n",
        "    f1_score = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    dice_coeff_avg = dice_coeff_sum / len(val_loader)\n",
        "    iou_avg = iou_sum / len(val_loader)\n",
        "    rand_error_avg = rand_error_sum / len(val_loader)\n",
        "    pixel_error = (FP + FN) / (total_pixels + 1e-6)\n",
        "\n",
        "    # Store metrics\n",
        "    precision_data.append(precision)\n",
        "    recall_data.append(recall)\n",
        "    accuracy_data.append(accuracy)\n",
        "    f1_data.append(f1_score)\n",
        "    dice_coeff_data.append(dice_coeff_avg)\n",
        "    iou_data.append(iou_avg)\n",
        "    rand_error_data.append(rand_error_avg)\n",
        "    pixel_error_data.append(pixel_error)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"IoU: {iou_avg:.4f}, F1 Score: {f1_score:.4f}, Dice Coefficient: {dice_coeff_avg:.4f}\")\n",
        "    print(f\"Pixel Error: {pixel_error:.4f}, Rand Error: {rand_error_avg:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9HiLKqyZVWE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 Testing\n",
        "\n"
      ],
      "metadata": {
        "id": "ltZioJY76zK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "precision_data_testing = []\n",
        "recall_data_testing = []\n",
        "accuracy_data_testing = []\n",
        "iou_data_testing = []\n",
        "f1_data_testing = []\n",
        "dice_coeff_data_testing = []\n",
        "rand_error_data_testing = []\n",
        "pixel_error_data_testing = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(config.device)\n",
        "        masks = masks.to(config.device)\n",
        "        outputs = model(images)\n",
        "        if isinstance(outputs, dict):\n",
        "            outputs = outputs['out']\n",
        "        outputs = outputs.squeeze(1)\n",
        "        loss = criterion(outputs, masks)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        tp = (preds * masks).sum().item()\n",
        "        fp = (preds * (1 - masks)).sum().item()\n",
        "        fn = ((1 - preds) * masks).sum().item()\n",
        "        tn = ((1 - preds) * (1 - masks)).sum().item()\n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        recall = tp / (tp + fn + 1e-6)\n",
        "        accuracy = (tp + tn) / (tp + fp + fn + tn + 1e-6)\n",
        "        iou = tp / (tp + fp + fn + 1e-6)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "        dice_coeff = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
        "        pixel_error = 1 - accuracy\n",
        "        rand_error = 1 - (tp + tn) / (tp + fp + fn + tn + 1e-6)\n",
        "        precision_data_testing.append(precision)\n",
        "        recall_data_testing.append(recall)\n",
        "        accuracy_data_testing.append(accuracy)\n",
        "        iou_data_testing.append(iou)\n",
        "        f1_data_testing.append(f1)\n",
        "        dice_coeff_data_testing.append(dice_coeff)\n",
        "        pixel_error_data_testing.append(pixel_error)\n",
        "        rand_error_data_testing.append(rand_error)\n",
        "test_loss /= len(test_dataset)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "avg_precision = sum(precision_data_testing) / len(precision_data_testing)\n",
        "avg_recall = sum(recall_data_testing) / len(recall_data_testing)\n",
        "avg_accuracy = sum(accuracy_data_testing) / len(accuracy_data_testing)\n",
        "avg_iou = sum(iou_data_testing) / len(iou_data_testing)\n",
        "avg_f1 = sum(f1_data_testing) / len(f1_data_testing)\n",
        "avg_dice_coeff = sum(dice_coeff_data_testing) / len(dice_coeff_data_testing)\n",
        "avg_pixel_error = sum(pixel_error_data_testing) / len(pixel_error_data_testing)\n",
        "avg_rand_error = sum(rand_error_data_testing) / len(rand_error_data_testing)\n",
        "\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average IoU: {avg_iou:.4f}\")\n",
        "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
        "print(f\"Average Dice Coefficient: {avg_dice_coeff:.4f}\")\n",
        "print(f\"Average Pixel Error: {avg_pixel_error:.4f}\")\n",
        "print(f\"Average Rand Error: {avg_rand_error:.4f}\")\n"
      ],
      "metadata": {
        "id": "og0o9_-BDoXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_data = {\n",
        "    \"precision\": precision_data_testing,\n",
        "    \"recall\": recall_data_testing,\n",
        "    \"accuracy\": accuracy_data_testing,\n",
        "    \"iou\": iou_data_testing,\n",
        "    \"f1\": f1_data_testing,\n",
        "    \"dice_coeff\": dice_coeff_data_testing,\n",
        "    \"pixel_error\": pixel_error_data_testing,\n",
        "    \"rand_error\": rand_error_data_testing\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Compute average metrics and append to the DataFrame\n",
        "average_row = {\n",
        "    \"precision\": sum(precision_data_testing) / len(precision_data_testing),\n",
        "    \"recall\": sum(recall_data_testing) / len(recall_data_testing),\n",
        "    \"accuracy\": sum(accuracy_data_testing) / len(accuracy_data_testing),\n",
        "    \"iou\": sum(iou_data_testing) / len(iou_data_testing),\n",
        "    \"f1\": sum(f1_data_testing) / len(f1_data_testing),\n",
        "    \"dice_coeff\": sum(dice_coeff_data_testing) / len(dice_coeff_data_testing),\n",
        "    \"pixel_error\": sum(pixel_error_data_testing) / len(pixel_error_data_testing),\n",
        "    \"rand_error\": sum(rand_error_data_testing) / len(rand_error_data_testing)\n",
        "}\n",
        "\n",
        "average_row_df = pd.DataFrame([average_row])\n",
        "results_df = pd.concat([results_df, average_row_df], ignore_index=True)\n",
        "results_df.loc[len(results_df) - 1, 'epoch'] = 'Average'\n",
        "results_df.to_csv(\"testing_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Vqn_ISKJ_F0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.3 Saving and loading the model"
      ],
      "metadata": {
        "id": "DuAc4yOzEQJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'Ynet34-atten_model.pth')\n",
        "\n",
        "# Load the model\n",
        "model.load_state_dict(torch.load('Ynet34-atten_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "UdT0PYRzEH6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, train_loss_data, label='Training Loss')\n",
        "plt.plot(epochs, val_loss_data, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over Epochs')\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, precision_data, label='Precision')\n",
        "plt.plot(epochs, recall_data, label='Recall')\n",
        "plt.plot(epochs, accuracy_data, label='Accuracy')\n",
        "plt.plot(epochs, f1_data, label='F1 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.legend()\n",
        "plt.title('Metrics over Epochs')\n",
        "plt.savefig('metrics.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, dice_coeff_data, label='Dice Coefficient')\n",
        "plt.plot(epochs, iou_data, label='IoU')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.legend()\n",
        "plt.title('Segmentation Metrics over Epochs')\n",
        "plt.savefig('segmentation_metrics.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, pixel_error_data, label='Pixel Error')\n",
        "plt.plot(epochs, rand_error_data, label='Rand Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.title('Error Metrics over Epochs')\n",
        "plt.savefig('error_metrics.png')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "eOfUCjqIBJvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get a batch of images and masks from the test loader\n",
        "images, masks = next(iter(test_loader_original))\n",
        "images = images.to(config.device)\n",
        "masks = masks.to(config.device)\n",
        "\n",
        "# Run the model on the images\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "    if isinstance(outputs, dict):\n",
        "        outputs = outputs['out']\n",
        "    outputs = outputs.squeeze(1)  # Remove channel dimension if necessary\n",
        "\n",
        "    # Apply sigmoid activation to get probabilities between 0 and 1\n",
        "    probs = torch.sigmoid(outputs)\n",
        "\n",
        "# Threshold probabilities to get binary masks\n",
        "pred_masks = (probs > 0.5).float()\n",
        "\n",
        "# Move data to CPU and convert to NumPy arrays for plotting\n",
        "images = images.cpu()\n",
        "masks = masks.cpu()\n",
        "pred_masks = pred_masks.cpu()\n",
        "\n",
        "# Denormalize images for visualization if you normalized them during preprocessing\n",
        "mean = np.array([0.485, 0.456, 0.406])  # ImageNet mean\n",
        "std = np.array([0.229, 0.224, 0.225])   # ImageNet std\n",
        "\n",
        "batch_size = images.shape[0]\n",
        "\n",
        "for i in range(batch_size):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original Image\n",
        "    img = images[i].permute(1, 2, 0).numpy()  # Change shape from [C, H, W] to [H, W, C]\n",
        "    #img = img * std + mean  # Denormalize\n",
        "    #img = np.clip(img, 0, 1)  # Clip values to [0, 1] range\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # Ground Truth Mask\n",
        "    gt_mask = masks[i].numpy()\n",
        "    ax[1].imshow(gt_mask, cmap='gray')\n",
        "    ax[1].set_title('Ground Truth Mask')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    # Predicted Mask\n",
        "    pred_mask = pred_masks[i].numpy()\n",
        "    ax[2].imshow(pred_mask, cmap='gray')\n",
        "    ax[2].set_title('Predicted Mask')\n",
        "    ax[2].axis('off')\n",
        "    #plt.savefig(f\"epoch_{epoch+1}_image_{i}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "YhlQzLzGErd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = {\n",
        "    \"epoch\": epochs,\n",
        "    \"train_loss\": train_loss_data,\n",
        "    \"val_loss\": val_loss_data,\n",
        "    \"precision\": precision_data,\n",
        "    \"recall\": recall_data,\n",
        "    \"accuracy\": accuracy_data,\n",
        "    \"iou\": iou_data,\n",
        "    \"f1\": f1_data,\n",
        "    \"dice_coeff\": dice_coeff_data,\n",
        "    \"rand_error\": rand_error_data,\n",
        "    \"pixel_error\": pixel_error_data\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(data)\n",
        "print(metrics_df)\n",
        "metrics_df.to_csv(\"training_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "60MQ98eL6f0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}